\chapter{RESULTS}

The main purpose of pursuing parallel methods for GIS processing
is to overcome the limitations of GIS processing using desktop
applications. This thesis uses PostGIS, the GIS extension for the
PostgreSQL relational database, to represent the capabilities of more
traditional GIS processing applications.

Given the capabilities and sophistication of a relational database,
record level operations performed in PostGIS are expected to outperform
the simpler algorithms used in hadoopGIS and clusterGIS. While there are
not limitations against using similarly sophisticated algorithms in the
parallel implementations, this thesis attempts to show what can be done
with relatively simple and naive algorithms, such as could be created
by people not possessing computer science degrees.

On the other hand, hadoopGIS and clusterGIS should outperform PostGIS in
operations requiring access to entire datasets. This is expected because
unless the dataset level operation can be done entirely using indexes,
which in the operations selected here is not the case, PostGIS is forced
to work with every record in the dataset; leaving it to a basic serial
algorithm. HadoopGIS and clusterGIS are able to process multiple records
at one time, therefore reducing processing time.

During the gathering of these results it was discovered that Hadoop,
and therefore hadoopGIS, is difficult to run with different numbers of
mappers. To achieve this, HDFS must be recreated with a block size that
implies the number of mappers that will be created. Each block of a file
gets a mapper. This methodology is not available in a production
environment.

Another problem with Hadoop is that, even with an appropriate blocksize,
was unable to run using a single mapper on the 1.2GB parcels dataset.
Because of this, it is not possible to accurately determine speedup.
Instead the runtime for the single core case will be derived from the
next nearest data point assuming 60\% efficiency. This allows for the
speedup curve to be drawn, but tends to shift it up. The analysis
for this thesis is more concerned with the shape of the speedup curve
than the efficiency, as efficiency is more directly tied to the specifics
of the implementations.

This chapter evaluates these assumptions and compares the relative
performance and scalability of hadoopGIS and clusterGIS by first
discussing characteristics of the record level operation experiments
and then the dataset level operations. After these assumptions are
evaluated, some remarks will be made on the ease of using each of these
three environments for GIS processing.

\section{Record Level Operations}

The classic operations to determine ability to perform record level
operations are to create a new record, read an existing record, update an
existing record, and delete an existing record from a dataset. Of these
four operations, create, update, and delete exhibited similar performance
characteristics in both PostGIS and the parallel implementations. Read
performance in the parallel implementations was markedly different.

As performance characteristics were clustered in this manner, the create
operation will be used to represent the performance of the create,
update, and delete operations. The read operation will represent itself.

\subsection{Create}

\begin{table}
\label{create-time}
\begin{center}
\begin{tabular}{lcc}
Implementation & Time (seconds) & Processor Cores\\
\hline
PostGIS & 0.012 & 1\\
hadoopGIS & 77.453 & 512\\
clusterGIS & 6.160 & 128\\
\end{tabular}
\caption{Fastest Execution Time for Create Operation}
\end{center}
\end{table}

\begin{figure}
\label{create-speedup}
\includegraphics{create-speedup.eps}
\caption{Create Speedup}
\end{figure}

The create operation inserts a record into a dataset. In the case of PostGIS, an existing dataset is mutated to include the new record. HadoopGIS and clusterGIS, as they work on the dataset level, each create a new dataset including all the records from the original dataset and the new record.

Table \ref{create-time} lists the fastest times that were executed in the series of tests along with the number of processors used. PostGIS is markedly faster than either of the parallel implementations. This is largely due to the fact that PostGIS is not required to both read and write the entire dataset, but only has to add the single record, confirm that the record does not violate any constraints of the dataset, and update any indexes. Even with the additional meta-work required, the significant difference between execution times allows for a large amount of additional work to be performed.

This number is for the addition of one record to a dataset. The parallel implementations have a large amount of overhead in reading and writing an entire dataset of 1.2 million records. Assuming that creating additional records was free, meaning each additional record added 0.000 seconds to the execution time, one would have to create 514 records using the faster of the parallel implementations, clusterGIS, to amortize the overhead.

Figure \ref{create-speedup} shows how hadoopGIS and clusterGIS perform with the addition of more computers. While the raw speed of hadoopGIS and clusterGIS differ greatly as shown in Table \ref{create-time}, the speedup graph helps us to ignore the differences in implementations of these these two technologies and see how additional resources can improve performance.

Both implementations are limited by I/O. Neither implementation has much computation to do, as they just read the data in and then write it back out with the additional record.

The speedup curves for both hadoopGIS and clusterGIS basically plateau after hitting the 16 core mark. This means that adding more resources past 16 cores is not worth the effort for this data size.

As was expected, PostGIS is able to outperform hadoopGIS and clusterGIS in creating records. While not tested, it appears that at least 514 records would have to be created in order to give the clusterGIS implementations a chance of outperforming PostGIS. HadoopGIS and clusterGIS scale similarly for these operations.

\subsection{Read}

The read operation is similar to the create operation for the parallel implementations in that the entire dataset must be read in. The difference is that only one record must be written to disk. PostGIS is able to use an index to quickly locate the requested record and then read just that data from disk and then output it to a new table. Table \ref{read-time} shows PostGIS to still be the fastest, as compared to the create operation, but that the parallel implementations are significantly faster even with the increased computation requirement of checking every single record to see if it was the record.

\begin{figure}
        \label{read-speedup}
        \includegraphics{read.eps}
        \caption{Read Speedup}
\end{figure}

The speedup shown in figure \ref{read-speedup} is higher than for the create operation due to the decreased I/O and slightly increased computation requirements. While the operations are still I/O bound, the difference is significant. Also note the point of diminishing returns is met much more quickly, at 16 cores for the clusterGIS implementation, than for the create operation. All of the experimental runs for clusterGIS have a peak at 16 cores, which indicates the datasize, distribution of data blocks on the Lustre filesystem setup, and network setup somehow have the best setup for redistributing the data.

\begin{table}
\label{read-time}
\begin{center}
\begin{tabular}{lcc}
Implementation & Time (seconds) & Processor Cores\\
\hline
PostGIS & 0.023 & 1\\
hadoopGIS & 49.324 & 8\\
clusterGIS & 2.015 & 16\\
\end{tabular}
\caption{Fastest Execution Time for Read Operation}
\end{center}
\end{table}

The speedup line for clusterGIS is similar to it's line for the create operation in that is mostly plateaus. The line for hadoopGIS, on the other hand, starts dropping off after 8 cores. Remember that the hadoopGIS speedup line is based on a estimated time for the single-core case that assumes the 8 core time is 60\% efficient. Remembering this prevents us from
making the incorrect observation that hadoopGIS always has a greater speedup than clusterGIS. This data cannot show that. Instead please note that the hadoopGIS implementation took 49 seconds at its fastest while clusterGIS took 2.

As expected, PostGIS is able to outperform hadoopGIS and clusterGIS in reading a single record from a dataset while using less resources. This is once again due to more sophisticated algorithms, data storage methods, and indexes. Again it is not unreasonable that such methods could be used to increase the performance of the parallel implementations. ClusterGIS scales better than hadoopGIS.

\section{Dataset Operations}

Now that basic performance and capabilities for record level operations have been established, and have been shown to be weaknesses for both parallel implementations, at least with the current naive and simple algorithms used, we come to the operations where the parallel implementations are expected to outperform PostGIS. These operations utilize entire datasets, first by filtering a dataset using a geometric computation, and then by utilizing two datasets in their entirety.

A scaling issue in hadoopGIS for the second dataset was encountered while running the nearest and chained operations. As a result, only the chained operation is show here with a description of the issue and how the clusterGIS implementation solves the same problem. ClusterGIS exhibits a similar speedup line for both the nearest and chained operations, and it is therefore assumed that if more memory were available that the hadoopGIS implementation's speedup would be similar to that of the chained operation. As such, only the chained operation is discussed here.

\subsection{Filter}

\begin{table}
\label{filter-time}
\begin{center}
\begin{tabular}{lcc}
Implementation & Time (seconds) & Processor Cores\\
\hline
PostGIS & 1.123 & 1\\
hadoopGIS & 50.408 & 8\\
clusterGIS & 2.957 & 128\\
\end{tabular}
\caption{Fastest Execution Time for Filter Operation}
\end{center}
\end{table}

\begin{figure}
	\label{filter-speedup}
	\includegraphics{filter.eps}
	\caption{Filter Speedup}
\end{figure}

The algorithms used for the filter operation are quite similar to those used in the read operation, but that instead of doing a simple comparison of record ids, a geometric computation is required. The intersection operation was chosen to represent any generic geometric operation in that it cannot utilize indexes currently available in PostGIS.

Comparing the execution times in table \ref{filter-time} to those of the read operation (table \ref{read-time}) we see that clusterGIS is able to absorb the additional computation requirements by adding more processing cores while only increasing computation time by one second. HadoopGIS performs in similar manner. PostGIS is the most hard hit by its inability to utilize additional resources, thus significantly increasing processing time.

Speedup for clusterGIS, as shown in figure \ref{filter-speedup} is much more near ideal speedup than the record level operations. The increased computation is able to offset the I/O requirements, reducing the effect of the I/O bottleneck and increasing efficiency. Furthermore, clusterGIS continues to scale until it hits the point of diminishing returns at 128 cores, which also happens to be the fastest time to complete this processing.

HadoopGIS's speedup line drops off almost immediately, showing that it does not scale well in this case.

As more computation is required to process each record, the parallel implementations are able to utilize more and more resources to reduce total computation time. PostGIS's execution time can only increase when more computation is required per record. ClusterGIS scales better than hadoopGIS in this case.

\subsection{Chained}

Finding the nearest parcel for each employer is a fairly normal GIS question to ask. The fastest execution times shown in table \ref{chained-time}, however, show significant differences in the capabilities of each implementation. ClusterGIS by far outperforms the other implementations.

\begin{table}
\label{chained-time}
\begin{center}
\begin{tabular}{lcc}
Implementation & Time (seconds) & Processor Cores\\
\hline
PostGIS & 198235.521 & 1\\
hadoopGIS & 87621.129 & 16\\
clusterGIS & 41.995 & 1024\\
\end{tabular}
\caption{Fastest Execution Time for Chained Operation}
\end{center}
\end{table}

A significant scaling problem was discovered the in architecture of the hadoopGIS implementation. The map reduce paradigm expects apply a map function to ever record in the input dataset. The nearest and chained operations require the use of a second dataset, the parcel dataset, to compare against for each employer. Therefore each execution of the map function requires full access to the parcel dataset. Hadoop provides the capability to load a copy of the parcel dataset into a mapper. Mappers are the part of the Hadoop architecture that executes the map functions. While the parcel dataset is not required to be read from disk for each map invocation, it is required to be available in the mapper. The problem comes in that the parcel dataset is quite large, with each mapper requiring 2.2 GB of RAM to store all the parcels except for residential parcels in memory. Because of the simplicity of the land use code matching this optimization was available for this case and was originally designed as the chained experiment.

While 2.2GB of RAM is not really that much, the computers used have a total of 16GB of RAM that is shared between 8 cores. This averages to 2GB of RAM per core (assuming no memory utilization by the operating system), which is less than the 2.2GB required. As each mapper executes a single thread of computation, 2 processors cores on each machine are left idle because of this memory requirement.

While it is possible to read in the entire parcel dataset for each map function, this is much slower due to the slow speed of disk as compared to memory. Another possible method to reduce memory usage, and thus increase the compute capabilities in this computation bound operation would be to read in the parcel data in chunks, finding local minimum distances for each employer in that chunk and then processing the next chunk and so on. This method could only be performed using the map-reduce paradigm by running multiple map-reduce processes. The first process would split the parcel dataset into manageable chunks, after which a process for each chunk would find local minimum for that chunk, and then a final process would reduce the local minimums to a global minimum. Such a process is possible to implement, but is somewhat unwieldily.

The next problem is due to the size of the employers dataset. When hadoopGIS uses 1024 cores, it needs to split the 2MB file into 1024 parts. ClusterGIS is able to split it into 128 parts due to its design that also splits up the parcels dataset which places the same employers on 8 different processors. Besides scaling the parcels dataset, clusterGIS can continue using a smaller primary dataset and maintain the ability to use more resources.

The clusterGIS solution to this same scaling problem is to modify how the data is split. The implementation used creates blocks of eight MPI tasks. Each block has a full copy of the parcel dataset, with each task in the block having just a portion. Instead of dividing the employer dataset between all the tasks, the tasks in each block contain the same set of employers. The block size is tunable such that the block size can be adjusted allowing for a specific implementation to be optimized for memory usage and problem granularity. This implementation was not tuned for optimal performance, but for explainability in that each computer has a full copy of the parcel dataset and a subset of the employers where each task has the same employers.

The speedup shown in figure \ref{chained-speedup} for clusterGIS is nearly linear for most of its run. The speedup starts to drop off starting at 256 cores, probably due to the small size of the employer dataset. At 1024 cores the employer dataset's 34,000 records are split into 128 sections. The increased work to split and the recombine the dataset starts to affect the granularity of the problem at that scale. A larger employer dataset or more computation per record would increase the number of cores required before this effect would be noticed. The performance is still impressive, converting each hour of execution time used by PostGIS into less than one second.

\begin{figure}
	\label{chained-speedup}
	\includegraphics{chained.eps}
	\caption{Chained Speedup}
\end{figure}

Though hadoopGIS experiences a severe limitation on the size of the parcel dataset, it is also able to produce results more quickly than PostGIS while having a reasonable speedup line.

As was expected, the parallel implementations are able to outperform PostGIS. Of the two, clusterGIS is able to scale computation capabilities along with the size of both datasets.

\section{Usability}

The usability of the GIS processing environments is important, though difficult to measure. This section addresses two areas of usability: execution, and programming.

\subsection{Execution}

Usability of execution address the difficulty in executing a GIS processing operation.

PostGIS based processing operations can be done in any way that PostgreSQL supports. The main interfaces are through programming libraries, the psql command line client, and the PgAdmin client. Both PgAdmin and psql are able to execute SQL queries that are stored in text files or entered interactively. SQL queries do not need to be compiled. PgAdmin also provides a method to browse and edit the data in tables. The programming libraries are the most sophisticated, but also the hardest to use.

HadoopGIS execution is done by compiling the Java source code for the processing operation into a jar file. It is also necessary for the processing operation to have a main function and implement the Tool interface. After the jar file is created, the processing can be launched from the command line using the 'hadoop jar' command. It is also possible to develop in the Eclipse environment which can handle the compilation and execution of Hadoop jobs.

ClusterGIS execution is done through the mpirun or mpiexec commands provided by the MPI environment. Usually this command is wrapped in a jobscript that is sent to a scheduler which allocates resources from the cluster for this specific processing run. The MPI execution engine works on compiled code.

In producing the experimental results for this thesis, PostGIS was the easiest to run. That is because PostGIS can only use one processor and so only the single setup is needed. ClusterGIS was next easiest to run. After gaining access to some of the cluster's resources, each processor run was able to be done in a simple for loop. Changing the amount of resources used is done by changing an argument to mpiexec.

HadoopGIS was by far the most difficult to run. Each different number of processors required a new HDFS setup with an appropriate block size so that the right number of mappers would be used. If there is no need to perform runs in this manner, meaning that only the output of the processing phase is important, and if the secondary dataset is sufficiently small, running on an existing Hadoop cloud is very easy. A Hadoop task is only hard to run when the user cares about the specifics of it resource usage.

\subsection{Programming}

Programming usability address the difficulty in developing the GIS processing operation.

PostGIS produced the shortest programs. The record operations and the filter operation were straightforward and easy to code up. The nearest and chaining operations, however, were not. The obvious solution that uses a nested query performed very badly. The optimized version used in this thesis took hours of work and experimentation to create. It is not obvious and requires thinking through how the execution should work, without much feedback from the system.

HadoopGIS is easy to work with assuming the map-reduce paradigm is understood and the processing operation works well in that paragidm. Operations like Filter are well matched to the paradigm and are easy to code. Operations like Chained work, and give speedup, but can produce errors such as running out of memory that are not easy to solve.

ClusterGIS is the most flexible in terms of what can be done. This flexibility also produces the opportunity for trouble. However, most of the operations implemented for this thesis follow the load, loop, write pattern. Loading and writing are done through helper functions. Looping currently requires an understanding of how to use linked lists. More advanced usage patterns like the scaling done in the nearest and chaining operations requires either the ability to understand what is happening, or the ability to copy and past the example and modify the interior parts.