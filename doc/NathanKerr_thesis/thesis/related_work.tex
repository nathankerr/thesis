\chapter{RELATED WORK}

GIS processing is not new, and a large body of work exists to drawn from. Here I discuss a few aspects of that body of work that are most applicable to this thesis. I start with the programmer's view of GIS - libraries. Then I move to serial and parallel shared-memory programs. Then I move to work done on parallel, distributed memory implementations. The main goal of parallel applications is to make use of multiple processors to complete the required processing more quickly. In short, the processors must work together to complete a single processing operation.

\section{GIS libraries}

The Open Geospatial Consortium (OGC) defined a core set of geospatial processing operations in their Simple Features\cite{ogc-sf} standard. These core operations allow for most geospatial processing needs. One of the main libraries that provides these operations and related data types is the Java Topology Suite\cite{jts} (JTS). Though written in Java, the JTS has been used as the basis for ports into other languages. The Geometry Engine, Open Source (GEOS) library is a C++ port of the JTS that also provides a C interface. PostGIS is implemented using the GEOS library. The NetTopologySuite\cite{nts} (NTS) is a port of the JTS into .NET.

As quality libraries are available for a variety of languages, there is no need to re-implement the functionality they provide. This thesis makes direct use of the JTS and GEOS libraries.

\section{Serial and Parallel Shared-Memory Applications}

Computer programs are executed by processors. The simplest of programs is made up of a series of operations that are executed in order by the processor. As this type of application is comprised by a series of operations it is known as a serial program. Serial programs are not able to take advantage of more than one processor. To utilize more than one processor at a time, a set of serial programs that can communicate with each other is required. Each serial program in this set is referred to as a thread. Thus multi-threaded programs can utilize more than one processing core. The simplest method of communication between threads is to share a common section of memory. Therefore a parallel shared-memory application could utilize at most the number of processors able to be connected to a single section of memory.

Both serial and parallel shared-memory applications are limited to a single computer, where computer means a group of processors that are connected to the same memory. Most modern computers provide this capability.

\begin{figure}
	%TODO
	\label{shared-memory}
	\caption{Shared-Memory Machine Architecture}
\end{figure}

The wide variety of desktop GIS processing applications such as ESRI's ArcGIS\cite{arcgis}, QuantumGIS\cite{qgis} and GRASS GIS\cite{grass} work on shared memory machines. While these programs provide GIS processing capabilities and graphical user interfaces, they are limited to working on a shared memory. Thus they are limited to a single computer's memory and processing capabilities.

Geospatial databases such as PostGIS\cite{postgis} and ArcSDE\cite{arcsde} also work on shared-memory machines, but with the intent of sharing the processing capabilities of the machine they run on. Because of my work on the Urban Systems Framework\cite{usf} (USF), I am most familiar with PostGIS. PostGIS is able to execute multiple queries at one time, but no single query uses more than one thread. Even if these databases could utilize all the processing capabilities of the computer to execute one query, they would still be limited to a single machine.

\section{Parallel Distributed-Memory Applications}

To overcome the limitations of a single computer, the more scalable architecture of a parallel distributed-memory machine was created. The basic unit of this architecture is a processing element (PE) comprised of one or more processors couple with memory. In other words, a PE is a shared-memory machine. The PEs are interconnected using some sort of networking technology. This architecture scales as well as the interconnect does. Common interconnect technologies in use today are Gigabit-Ethernet and InfiniBand. Clusters are parallel distributed-memory machines.

\begin{figure}
	%TODO
	\label{distributed-memory}
	\caption{Distributed-Memory Machine Architecture}
\end{figure}

For a program to run on a parallel distributed-memory machine, it must be able to work with multiple thread of operation where communication between the threads goes over the interconnect. For the purposes of discussion, threads running on different PEs are called tasks.

The main task in designing a parallel program is figuring out how to split up the work between tasks. One method is to split up the processing between tasks. Each task would generally have the same data and perform variations of an operation on the data; for instance, running multiple scenarios. This methodology is called Task Parallel. An example of task parallelism is simulating the effects of various weather patterns on an urban environment. Each task would have a copy of the environment and run its variety of weather on it. The capability of executing each scenario is limited to the capabilities of a single processing element, but many scenarios can be executed at the same time. 

Data Parallel methods split the data up between tasks an perform the same, or similar, operations on each piece of data. To calculate the effects of a weather pattern on an urban environment, the environment would first be divided between the tasks with each task responsible for one part of the environment. Each task would then calculate the effects of the weather on its section of the environment, communicating with the other PEs as needed to share information related to edge conditions, etc.

\subsection{Parallel Databases}

Parallel databases such as TeraData\cite{teradata} and Oracle\cite{oracle} use data parallel methods. Paradise\cite{paradise, cs-paradise} spreads data 
between computers using round-robin, hash, or spatial partitioning. Because
the data is able to be distributed between multiple computers, the
processing is able to scale to larger datasets.

When a query is processed across the database, a task is created
for each fragment of the data. Thus as the data grows larger, the
processing capabilities of the system also increase. If a particular
processing operation requires relatively less computation for each
data record this is fine. However, after the amount of computation per
data record increases beyond a certain point, which is dependent on the
speed of the machine used, the processing operation can be sped up by
utilizing more processors. Major factors in determining how much data
should be processed on each machine, and therefore how the data should
be spread between machines, are memory, computation, and communication
overhead to move the data and computation to another machine. The ratio
of computation to memory and commutation requirements is often referred
to as grain size. Coarser grained processes have more computation per
data record, while finer grained processes have little computation for
each data record.

Databases excel at working with indexed data while allowing multiple
users to interact with the data in a concurrently safe manner through
the use of atomic transactions. The requirements placed upon database
systems to handle these situations slow down computations that don't
utilize indexes or work on an entire dataset at once. The processing
operations this research examines do not require these restrictions,
and as such a more efficient system can be created.

Many universities and research institutions already have significant
investment in compute clusters. These clusters are groups of computer
linked together with high speed network interconnects and high performance
parallel filesystems such as Lustre\cite{lustre}. By separating compute
and storage resources at the cost of a high speed network, compute
clusters are able to separate computation from data storage.

Parallel filesystems allow the data to be separated from
the computer where the processing will be executed by spreading files
across multiple network connected fileservers allowing access that
can be faster than utilizing a computer's local disk for storage
while also enabling processing to spread across the available compute
resources based entirely on the process' grain size.

\subsection{Parallel GRASS GIS}

GRASS GIS was
reworked\cite{pgrass} to use its collaboration features to distribute
sub-queries among computers. The method described in this paper utilizes
multiple instances of GRASS in a master-slave configuration where
all participants access a shared data repository or filesystem. The
geometries are portioned between the various nodes. Operations are done
on the subsets, and the results are merged to produce the final result.

While the method used to extend GRASS GIS will in fact speed up GIS
processing, it has two flaws. First, GRASS is designed to be used in an
interactive mode rather than a batch or script driven approach.  Second,
the entire set of data used must still fit on a single computer to move
it in or out of the environment.