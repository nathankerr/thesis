\chapter{EXPERIMENTAL SETUP}

Chapter 3 defined the requirements for a good parallel GIS processing
engine. Chapter 4 discussed the designs used in hadoopGIS and
clusterGIS. Given these requirements and designs, the implementations
can now be evaluated and compared against each other. As both these
implementations are experimental, PostGIS will be used as a reference
point to represent current methods. As such, it will also be evaluated
as much as possible in the same manner as hadoopGIS and clusterGIS.

\section{Standard Geospatial Operations}

The first requirement is that an implementation can perform a set of
standard geospatial operations. Allowing full access to a compliant
geospatial library fulfills this requirement a priori. PostGIS uses,
and in fact developed, the GEOS library. HadoopGIS makes use of JTS,
from which the GEOS library was ported. ClusterGIS uses the GEOS library.

All three implementations fulfill this requirement, no further
experimentation is needed.

\section{Batch Mode Processing}

Batch mode operation is essential to running operations on a cluster.

PostGIS extends the PostgreSQL database. PostgreSQL can be accessed
through many different methods including programming interfaces in
C and other languages, or through the provided command line client,
``psql''. This client can be used interactively, or a sql script can be
passed to it. That leaves the problem of the server. Either a dedicated
server can be used, or a script can be written to create a server as
needed to do the processing. This thesis sets a PostgreSQL server up as
needed. The more normal use case for PostgreSQL is to have a dedicated
server always running to handle the required processing. PostGIS can
perform batch mode processing.

Hadoop executes user jobs from a command line interface. The only
hindrance for Hadoop to batch mode processing is the same as PostGIS:
Hadoop is designed to have a persistent server environment setup. Projects
like Hadoop on Demand allow a Hadoop environment to be created as
needed. This thesis accomplishes a similar solution through a custom
designed script.  With this setup, HadoopGIS is capable of batch mode
processing.

ClusterGIS only operates in a batch mode, there is no interactive mode.

\section{Scalability}

The first two requirements did not require any experimentation to
evaluate. Scalability requires experimentation. This section details the
experiments and specific implementation details. Chapter 6 presents and
discusses the results of these experiments.

Verification of operations will be done by comparing output of each
operation for HadoopGIS and ClusterGIS with the output from a PostGIS
reference implementation.

As HadoopGIS and ClusterGIS are processing engines, scalability depends
on the individual processing that is done. To achieve this, a set of
processing operations is defined. After the operations are discussed in
general, specific details for each implementation are discussed.

\subsection{Operation set}

\begin{table}
\begin{center}
\begin{tabular}{lcc}
Section & 1 Geometry & 2 Geometries\\
\hline
6.1.2 (Geometry) & 16 & 15\\
6.1.4 (Point) & 4 & 0\\
6.1.6 (Curve) & 5 & 0\\
6.1.7 (LineString, Line, LinearRing)& 2 & 0\\
6.1.8 (MultiCurve) & 2 & 0\\
6.1.10 (Surface) & 3 & 0\\
6.1.11 (Polygon, Triangle) & 3 & 0\\
6.1.12 (PolyhedralSurface) & 4 & 0\\
6.1.13 (MultiSurface) & 3 & 0\\
6.1.15 (Relational Operators) & 0 & 9\\
\hline
Total & 42 & 24\\
\end{tabular}
\caption{OGC Methods by Number of Required Geometries}
\label{MethodCount}
\end{center}
\end{table}
\addtocontents{lot}{\protect{\vspace{-12pt}}}

One of the main tasks in parallelizing an algorithm is solving the
problem of data access. HadoopGIS and clusterGIS split up the data that
is being processed between different computers. The OGC SFS\cite{ogc-sfs} standard
defines the operations that need to be supported. Table \ref{MethodCount}
shows a count of the number of operations in each section of the standard
that defines operations along with the number of geometries required to
execute that operation. One geometry is required for 42 of the operations,
while 24 operations require two. There are no operations that require
more than two geometries.

The problem faced by HadoopGIS and ClusterGIS then is to get the data
required to perform the geospatial operations required by what ever
processing is required. The operations defined here are meant to be
representative of what actual processing requirements would demand.
The first four operations deal with individual record access. The
following three operations deal with getting the data needed for a
geospatial operation where it is needed.

Specific details on the datasets follow the operation descriptions.
For now all that is needed to know is that there are two datasets,
one of 34 thousand employers and one of 1.2 million parcels of land.

\subsubsection{Create}

The create operation adds a new record to an existing
dataset. Adding a record is the basic operation required to build a new
dataset. Record-centric systems merely add a record to the record store,
while dataset-centric systems output a new dataset that contains the data
of the input dataset with the new record included in it.  Expected output
is the original dataset with the addition of a single record.

The create operation implementations add a parcel of land to the parcels
dataset.

\subsubsection{Read}

The read operation extracts a single record from a dataset based on a
unique identifier.  Record-centric systems are able to employ indexes and
other methods to quickly locate a record whereas dataset-centric systems
must scan an entire dataset to produce the single record. Expected output
is a single record.

The read operation extracts a parcel from the parcels dataset.

\subsubsection{Update}

The update operation finds a single record from a dataset and changes
some attribute of the record.  Record-centric systems are able to employ
indexes to locate and modify the record. Dataset-centric systems generate
a new dataset with the changed record. Expected output is a dataset that
contains all the records from the input dataset with exception that the
specified record has been changed in the specified manner.

The update operation changes the land use code for a parcel in the
parcels dataset.

\subsubsection{Destroy}

The destroy operation finds and removes a record from a
dataset. Record-centric systems are able to find the record to be
removed and then removing it from the record store. Dataset-centric
systems generate a new dataset without the specified record. Expected
output is a dataset with all the records from the input dataset except
for the one that was removed.

The destroy operation removes a parcel from the parcels dataset.

\subsubsection{Filter}

The filter operation removes all records that don't fulfill a certain
requirement, in this case all the records that don't intersect with
a defined geometry. Record-centric systems do not have much of an
advantage here as indexing the results of some geospatial operation
is not usually worth the indexing cost, so both the record-centric and
dataset-centric systems must scan the entire dataset. Expected output
is a dataset containing all of the records from the input dataset that
fulfill the filtering requirements.

The filter operation removes all parcels of land that don't overlap a
defined region in the parcels dataset.

\subsubsection{Nearest}

The nearest operation is derived from an actual processing operation
required for the Digital Phoenix Project\cite{digitalphoenix}. This
operation uses two datasets. The first is a set of points representing
employers in the Phoenix metro-area. The second is a set of polygons
representing parcels of land in the same area. Digital Phoenix needed
to match the employer with the parcel of land where the business
should be. Thus the operation is for each employer, find the nearest
parcel of land with a compatible zoning code. The datasets used in this
thesis have been simplified to make this matching appear more straight
forward. Expected output is a list of employer ids with associated
parcel ids.

The nearest operation uses both the employers and parcels datasets.

\subsubsection{Chaining}

The chaining operation combines the filter and nearest operations with
the intent of showing how multiple operations can be performed one after
the other. This operation is an obvious optimization of the nearest
operation in that it first removes all residential parcels before running
the nearest operation on the remaining parcels. As employers cannot (in
this simplified world) exist on residential parcels, and since residential
parcels make up a large portion of the entire parcel dataset, the number
of distances calculated for each employer will be significantly decreased
thus decreasing processing time. Expected output is the same as for the
nearest operation.

The chaining operation uses both the employers and parcels datasets.

\subsection{Dataset Description}

Two datasets are used in evaluating these operations. The first is a
set of employers where each record contains an employer id, the place
where the employer is located represented as a point, and the business
classification: commercial, industrial, or governmental. The employer
dataset contains 34,302 records.

The second dataset is a set of parcels where each record contains a parcel
id, a multipolygon representing the parcel coverage, and a land use code:
residential, commercial, industrial, or governmental. The parcel dataset
contains 1,218,130 records.

Both datasets use R, C, I, and G to represent residential, commercial,
industrial, and governmental use codes. These datasets are simplified
versions of real datasets for Maricopa County, Arizona. The datasets
were simplified by adding the simplified land use code attribute and
removing the other attributes not used in these operations. The same
datasets were used in all environments.

\section{Execution Environment}

All operations will be executed on ASU's Saguaro cluster. The Saguaro
cluster is comprised of several generations of hardware. To simplify
comparisons, all operations will be executed on similar hardware with
similar network interconnects, using the one most natural for use.
This means that PostGIS and hadoopGIS use the gigabit ethernet interfaces,
as these services are normally deployed this way. ClusterGIS uses the Infiniband
interface for access to the Lustre filesystem.

Saguaro is a CentOS 5.3 based cluster running on Intel Xeon
processors. Each node used in these experiments has two harpertown
processors each with four cores running at 2.83GHz and has 16GB of
RAM. In addition each node has one gigabit ethernet connection and one
DDR InfiniBand connection. The shared filesystem for these experiments
is Lustre 1.6.6 accessed through the InifiniBand connection.

\section{PostGIS Implementation}

The PostGIS experiments were ran using PostgreSQL 8.4.1 and PostGIS
1.3. Both pieces of software were compiled for these tests and used
default options.

Each of the following operations was executed on a freshly setup PostGIS
instance, running on local disk with default options. Data was loaded,
indexed, and vacuumed before the experiment was executed. Runtimes were
gathered by using the linux `time' command. When a query's result did
not modify an existing table, its results were stored into another table.

All data storage for PostGIS was done on the local hard disk.

Full SQL statements for each operation are listed below.

\subsection{Create}

The create operation for PostGIS as shown in listing \ref{postgis-create}
is a simple SQL insert statement.

\begin{lstlisting}[caption={PostGIS Create Operation},label={postgis-create},language=]
insert into parcels values (97123897, ST_GeomFromText('POLYGON((-112.0859375 33.4349975585938,-112.0859375 33.4675445556641,-112.059799194336 33.4675445556641,-112.059799194336 33.4349975585938,-112.0859375 33.4349975585938))', 4326), 'C');
\end{lstlisting}

The most complicated portion of this operation is creating the geometry
type. PostGIS provides several functions that can create geometry types
from text representations. The one used here, ST\_GeomFromText, creates
a geometry from the standard Well Know Text format.

\subsection{Read, Update, and Destroy}

The read, update, and destroy operations use the basic SQL select,
update, and delete statements. Listing \ref{postgis-update} shows the
update operation which is representative of all three of these operations.

\subsection{Update}
\begin{lstlisting}[caption={PostGIS Update Operation},label={postgis-update},language=]
update parcels set devtype = 'C' where id = 1008130;
\end{lstlisting}

\subsection{Filter}

The Filter operation shown in Listing \ref{postgis-filter} is the first
of these operations to take advantage of the processing capabilities of PostGIS.

\begin{lstlisting}[caption={PostGIS Filter Operation},label={postgis-filter},language=]
create table output as
select parcels.* from parcels, (select ST_GeomFromText('POLYGON((-112.0859375 33.4349975585938,-112.0859375 33.4675445556641,-112.059799194336 33.4675445556641,-112.059799194336 33.4349975585938,-112.0859375 33.4349975585938))', 4326) as the_geom) as box
where ST_Intersects(parcels.the_geom, box.the_geom);
\end{lstlisting}

The operation returns all the records that intersect with a predefined
geometry, in this case created using ST\_GeomFromText. ST\_Intersects
was chosen to perform the intersection because it does not utilize the
bounding box based index provided by PostGIS. As the operation is not
influenced by that index, which can only be used for a few operations,
this query is more representative of real-world conditions and more
complicated filtering queries.

\subsection{Nearest and Chaining}

The chaining operation is defined as comparing all employers to all parcels except for those parcels with a residential land-use code. PostGIS automatically performs this optimization when the land-use code field is indexed, therefore the nearest and chained operations are equivalent.

\begin{lstlisting}[caption={PostGIS Nearest and Chained Operations},label={postgis-chained},language=]
begin;

-- Figure out the distance to all applicable parcels
create index jobs_devtype_index on jobs using btree ("devtype");
create index parcels_devtype_index on parcels using btree ("devtype");
create temp table distances on commit drop as
select jobs.id as job_id, parcels.id as parcel_id, distance(jobs.the_geom, parcels.the_geom)
from jobs, parcels
where jobs.devtype = parcels.devtype;

-- Find the distance to the closest parcel
create temp table min on commit drop as
select job_id, min(distance) as distance
from distances
group by job_id;

-- Match the job to the closest parcels
create index distances_distance_index on distances using btree ("distance");
create index min_distance_index on min using btree ("distance");
create temp table nearest on commit drop as 
select distances.job_id, distances.parcel_id
from distances, min
where distances.job_id = min.job_id
and distances.distance = min.distance;

-- Only use unique job_id's 
create index nearest_job_id_index on nearest using btree ("job_id");
create table "jobs_parcels" as 
select nearest.job_id, nearest.parcel_id
from nearest
where nearest.parcel_id = (select n.parcel_id from nearest as n where n.job_id = nearest.job_id limit 1);

commit;
\end{lstlisting}

When developing this operation, it was hoped that a single
nested query would be sufficient. Unfortunately the nested query
performed significantly slower than the optimized query shown in Listing
\ref{postgis-chained}. Indexes beyond the standard primary key index are
created as part of this query.

The first step in the query process is to create a distance table between
every job and ever parcel with matching devtypes. The next step determines
what the minimum distance is for each employer. The third step matches
the previously found minimum distance with all the parcels that are
that distance away. The final step reduces the number of parcels that
are equidistant (using the minimum distance) from each employer to one.

\section{HadoopGIS Implementation}

The hadoopGIS experiments were executed using Hadoop 0.19.0, data was
accessed on an HDFS running on the same nodes off local disk. Internode
communication was accomplished through Gigabit Ethernet.

Default values were mostly used. Changes were made to increase the number
of mappers per node to 8, increasing the memory available to each JVM,
and altering the HDFS block size to produce the desired number of mappers.

Core algorithm details are described below. In all cases the reduce
function is an identity function where all input is passed through as
output, therefore the including listings in this section display code that
is part of the map function. Full listings are available in the appendix.

\subsection{Create}

The Hadoop architecture makes it difficult to arbitrarily output
a record. The processing method is fully dependent on the input
records. Since it was not possible to add a record to the output
dataset without relying on some input, like is possible for PostGIS and
clusterGIS, this operation creates the new record when triggered by a
record that already exists in the dataset. The resulting map function
is shown in Listing \ref{hadoopgis-create}.

\begin{lstlisting}[caption={HadoopGIS Create Operation},label={hadoopgis-create},language=]
if(key == 1008130) {
        GIS created = new GIS();
        GIS.update("\"97123897\",\"POLYGON((-112.0859375 33.4349975585938,-112.0859375 33.4675445556641,-112.059799194336 33.4675445556641,-112.059799194336 33.4349975585938,-112.0859375 33.4349975585938))\",\"C\"");
        output.collect(new LongWritable(new Integer(97123897)), created);
}
output.collect(key, value);
\end{lstlisting}

\subsection{Read, Update, and Destroy}

The read, update, and destroy operations are very similar. Listing \ref{hadoopgis-update} shows the simplicity of the update operation. All three operations use a little bit of logic to determine if a record is passed onto the reduce phase. The read operation only keeps the requested record; update modifies a specific record before passing it on, and destroy passes on all the records except for the one specified.

\subsection{Update}
\begin{lstlisting}[caption={HadoopGIS Update Operation},label={hadoopgis-update},language=]
if(key.equals(new LongWritable(1008130))) {
        value.attributes.put("devtype", "C");
}
output.collect(key, value);
\end{lstlisting}

\subsection{Filter}

The filter operation works in a similar manner as the read, update,
and destroy operations as shown in Listing \ref{hadoopgis-filter}.

\begin{lstlisting}[caption={HadoopGIS Filter Operation},label={hadoopgis-filter},language=]
if(value.geometry.intersects(box)) {
        output.collect(key, value);
}
\end{lstlisting}

The intersects function requires a geometry. To keep from recreating
the geometry filtered against, the geometry is created in the configure
method which is ran at the creation of the mapper, and therefore is only
referenced in the map function.

\subsection{Nearest and Chaining}

The nearest operation uses the employers dataset as input for the map
functions. As the operations compares every employer against every parcel,
the parcel dataset is loaded in its entirety in the configure method
(ran when creating each mapper). This way of using a secondary dataset
is not optimal, but it is the only method allowed in the map-reduce
framework. Being forced to include a full copy of the parcels dataset
increases the memory requirement for each mapper, thus limited the number
of mappers that can be ran on a single machine and the maximum size of
the parcel dataset. The clusterGIS implementation presents a solution
for this problem that is possible in MPI but not Hadoop.

Even with that problem, the resulting map function is quite simple as
shown in Listing \ref{hadoopgis-nearest}.

\begin{lstlisting}[caption={HadoopGIS Nearest Operation},label={hadoopgis-nearest},language=]
double minDistance = Double.MAX_VALUE, currDistance;
int closestParcel = -1;

Iterator it = parcels.iterator();
while (it.hasNext())
{
        GIS parcel = (GIS) it.next();

        currDistance = value.geometry.distance(parcel.geometry);
        if (currDistance < minDistance)
        {
                minDistance = currDistance;
                closestParcel = new Integer(parcel.attributes.get("id"));
        }
}

LongWritable lngClosestParcel = new LongWritable (closestParcel);
output.collect(key, lngClosestParcel);
\end{lstlisting}

The map function uses a simple algorithm to find the minimum distance
to a parcel and then outputs the id of both the employer and the parcel.

The chaining operation reduces the memory problems encountered in the
nearest operation implementation by removing all residential parcels. This
optimization works for this specific problem but cannot be generally
applied. As this optimization is applied during the mapper's configure
function, the algorithm used to find the nearest parcel does not change.

\section{ClusterGIS Implementation}
\lstset{language=C}

ClusterGIS was compiled with the Intel C Compiler (icc) 10.1 20080312
against MVAPICH 1.0.1.  MVAPICH makes use of the DDR InfiniBand network
connection for communication. Dataset storage is done on Lustre, which
is connected through the same InifiBand connections.

The specific main portion of each operations algorithm is show
below. Dataset loading and writing is accomplished using the distributed
method including all tasks in the operation unless otherwise stated. Full
code listings are available in the appendix.

\subsection{Create}

Unlike the hadoopGIS create operation, it is easy to arbitrarily add
a record using clusterGIS. In this case the first tasks creates a new
record using a helper function provided by clusterGIS and then inserts
it into its local dataset. When the dataset is written collectively,
the new record is included.

\begin{lstlisting}[caption={ClusterGIS Create Operation},language=]
if(rank == 0) {
        int start = 0;
        record = clusterGIS_Create_record_from_csv("97123897,POINT(0 0),C\n", &start);
        record->next = dataset->data;
        dataset->data = record;
}
\end{lstlisting}

\subsection{Read, Destroy}

The read and destroy operations are implemented in a similar manner. Listing \ref{clustergis-read}
shows the main portion of the read operation.

\begin{lstlisting}[caption={ClusterGIS Read Operation},label={clustergis-read},language=]
record = dataset->data;
head = &(dataset->data);
/* keep records that match the criteria, otherwise delete them */
while(record != NULL) {
        if(atoi(record->data[0]) == 1008130) {
                head = &(record->next);
                record = record->next;
        } else {
                *head = record->next;
                clusterGIS_Free_record(record);
                record = *head;
        }
}
\end{lstlisting}

This implementation looks more complicated than the corresponding
hadoopGIS operations, but performs the same processing. ClusterGIS
provides all the records in a dataset as a linked list, which is not
as pretty to manipulate as the Java container objects.

This code loops through each record in the linked list. The record
variable points to the current record. The head variable points to the
pointer in the previous record that points to the current record. To keep
a record, such as is done in the then block of the if statement, it is
only necessary to update these variable to the next record. To delete
a record, head is set to point to the next record and then the current
record is deleted from memory. Record is then advanced to the next record.

\subsection{Update}

The update operation as shown in Listing \ref{clustergis-update} shows
the simplest way to loop through a linked list. Each record is checked
to see if it matches the criteria, and then the record is updated.

\begin{lstlisting}[caption={ClusterGIS Update Operation},label={clustergis-update},language=]
record = dataset->data;
/* keep records that match the criteria, otherwise delete them */
while(record != NULL) {
        if(atoi(record->data[0]) == 1008130) {
                record->data[2] = "C";
        }


        record = record->next;
}
\end{lstlisting}

\subsection{Filter}

The filter operation as shown in Listing \ref{clustergis-filter} builds on
the structure used in the read operation. Instead of comparing against
an id number, the filter operation uses a geometric intersection to
determine if the record should be kept. The shown code can easily be
extended to use an arbitrarily complex filtering parameter.

\begin{lstlisting}[caption={ClusterGIS Filter Operation},label={clustergis-filter},language=]
box = GEOSWKTReader_read(reader, "POLYGON((-112.0859375 33.4349975585938,-112.0859375 33.4675445556641,-112.059799194336 33.4675445556641,-112.059799194336 33.4349975585938,-112.0859375 33.4349975585938))");

record = dataset->data;
head = &(dataset->data);
/* keep records that match the criteria, otherwise delete them */
while(record != NULL) {
        char intersects;

        intersects = GEOSIntersects(record->geometry, box);

        if(intersects == 1) { /* record overlaps with box */
                head = &(record->next);
                record = record->next;
        } else { /* no overlap */
                *head = record->next;
                clusterGIS_Free_record(record);
                record = *head;
        }
}
\end{lstlisting}

The two main differences show how to create a new geometry object using
GEOSWKTReader and how to use the GEOSIntersects function. Beyond these
differences the loop structure is the same.

\subsection{Nearest and Chaining}

Upto this point, all the operations for clusterGIS have only had
to work on one dataset. The nearest and chained operations work on
two. Unlike hadoopGIS, clusterGIS is able to split up both datasets
between participating tasks. The tested implementation places a full copy
of the parcels dataset on each computer, which has eight tasks running on
it, where each of those tasks has the same set of employers. This means
that each task will find the closest parcel for each employer of the
parcels that it has (i.e., a local minimum). The tasks sharing the same
employers then talk amongst themselves to determine the globally nearest
parcel. This communication is done through MPI's reduce mechanism. The
employer id now associated parcel id are added to the output dataset.

This scheme for distributing both datasets was chosen for convenience 
of discussion. More sophisticated methods that dynamically take into
account data sizes and other heuristics are also possible and have
potential.

The nearest operation implementation and the chaining operation implementation
are very similar. Listing \ref{clustergis-chaining} shows the chaining operation.

\begin{lstlisting}[caption={ClusterGIS Chaining Operation},label={clustergis-chaining},language=]
/* remove all residential parcels */
parcel = parcels->data;
head = &(parcels->data);
while(parcel != NULL) {
        if(strncmp(parcel->data[2], "R", 1) == 0) {
                head = &(parcel->next);
                parcel = parcel->next;
        } else {
                *head = parcel->next;
                clusterGIS_Free_record(parcel);
                parcel = *head;
        }
}

/* Find the min distance */
employer = employers->data;
min = malloc(sizeof(double)*2);
global_min = malloc(sizeof(double)*2);
output = clusterGIS_Create_dataset();
while(employer != NULL) {

        /* find the local min */
        parcel = parcels->data;
        GEOSDistance(employer->geometry, parcel->geometry, &min_distance);
        min_distance_parcel = parcel;
        while(parcel != NULL) {
                if(strncmp(employer->data[2], parcel->data[2], 1) == 0) {
                        GEOSDistance(employer->geometry, parcel->geometry, &distance);
                        if(distance < min_distance) {
                                min_distance = distance;
                                min_distance_parcel = parcel;
                        }
                }
                parcel = parcel->next;
        }

        /* find the global min */
        min[0] = atoi(min_distance_parcel->data[0]);
        min[1] = min_distance;
        MPI_Allreduce(min, global_min, 2, MPI_DOUBLE, min_distance_op, parcels_comm);

        /* Add the min to the output dataset using front insertion*/
        sprintf(output_csv, "\"%s\",\"%d\"\n", employer->data[0], (int) global_min[0]);
        start = 0;
        output_record = clusterGIS_Create_record_from_csv(output_csv, &start);
        output_record->next = output->data;
        output->data = output_record;

        employer = employer->next;
}
\end{lstlisting}

The difference between nearest and chaining is the addition of the first
loop, which removes all residential parcels from the parcel dataset. With
the exception of this loop, the rest of the listing reflects the nearest
operation's implementation.

The main loop finds the globally nearest parcel for each employer. Each
task first finds a locally nearest parcel using the GEOSDistance function
to calculate distance. The nearest parcel id and distance are store in
the min array, which is then used to find the global minimum.

The MPI\_Allreduce command with the help of the min\_distance\_op,
determines the minimum distance of all the min arrays for the tasks
which share the same set of employers. After the function finishes,
each task in the group have the same values their min array. This is
the global minimum.

The last section of this code creates new records which are added to
the output dataset. This dataset can then be used just like any other
dataset loaded in clusterGIS.
