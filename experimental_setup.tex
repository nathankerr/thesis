\chapter{EXPERIMENTAL SETUP}

Chapter 3 defined the requirements for a good parallel GIS processing
engine. Chapter 4 discussed the designs used in hadoopGIS and
clusterGIS. Given these requirements and designs, the implementations
can now be evaluated and compared against each other. As both these
implementations are experimental, PostGIS will be used as a reference
point to represent current methods. As such, it will also be evaluated
as much as possible in the same manner as hadoopGIS and clusterGIS.

\section{Standard Geospatial Operations}

The first requirement is that an implementation can perform a set of
standard geospatial operations. Allowing full access to a compliant
geospatial library fulfills this requirement a priori. PostGIS uses,
and in fact developed, the GEOS library. HadoopGIS makes use of JTS,
from which the GEOS library was ported. ClusterGIS uses the GEOS library.

All three implementations fulfill this requirement, no further
experimentation is needed.

\section{Batch Mode Processing}

Batch mode operation is essential to running operations on a cluster.

PostGIS extends the PostgreSQL database. PostgreSQL can be accessed
through many different methods including programming interfaces in
C and other languages, or through the provided command line client,
``psql''. This client can be used interactively, or a sql script can be
passed to it. That leaves the problem of the server. Either a dedicated
server can be used, or a script can be written to create a server as
needed to do the processing. This thesis sets a PostgreSQL server up as
needed. PostGIS can perform batch mode processing.

Hadoop executes user jobs from a command line interface. The only
hindrance for Hadoop to batch mode processing is the same as PostGIS:
Hadoop is designed to have a persistent server environment setup. Projects
like Hadoop on Demand allow a Hadoop environment to be created as
needed. This thesis accomplishes a similar solution through a custom
designed script.  With this setup, HadoopGIS is capable of batch mode
processing.

ClusterGIS only operates in a batch mode, there is no interactive mode.

\section{Scalability}

The first two requirements did not require any experimentation to
evaluate. Scalability requires experimentation. This section details the
experiments and specific implementation details. Chapter 6 presents and
discusses the results of these experiments.

Verification of operations will be done by comparing output of each
operation for HadoopGIS and ClusterGIS with the output from a PostGIS
reference implementation.

As HadoopGIS and ClusterGIS are processing engines, scalability depends
on the individual processing that is done. To achieve this, a set of
processing operations is defined. After the operations are discussed in
general, specific details for each implementation are discussed.

\subsection{Operation set}

\begin{table}
\begin{center}
\begin{tabular}{lcc}
Section & 1 Geometry & 2 Geometries\\
\hline
6.1.2 (Geometry) & 16 & 15\\
6.1.4 (Point) & 4 & 0\\
6.1.6 (Curve) & 5 & 0\\
6.1.7 (LineString, Line, LinearRing)& 2 & 0\\
6.1.8 (MultiCurve) & 2 & 0\\
6.1.10 (Surface) & 3 & 0\\
6.1.11 (Polygon, Triangle) & 3 & 0\\
6.1.12 (PolyhedralSurface) & 4 & 0\\
6.1.13 (MultiSurface) & 3 & 0\\
6.1.15 (Relational Operators) & 0 & 9\\
\hline
Total & 42 & 24\\
\end{tabular}
\caption{OGC Methods by Number of Required Geometries}
\label{MethodCount}
\end{center}
\end{table}

One of the main tasks in parallelizing an algorithm is solving the
problem of data access. HadoopGIS and clusterGIS split up the data that
is being processed between different computers. The OGC SFS standard
defines the operations that need to be supported. Table \ref{MethodCount}
shows a count of the number of operations in each section of the standard
that defines operations along with the number of geometries required to
execute that operation. One geometry is required for 42 of the operations,
while 24 operations require two. There are no operations that require
more than two geometries.

The problem faced by HadoopGIS and ClusterGIS then is to get the data
required to perform the geospatial operations required by what ever
processing is required. The operations defined here are meant to be
representative of what actual processing requirements would demand.
The first four operations deal with individual record access. The
following three operations deal with getting the data needed for a
geospatial operation where it is needed.

Specific details on the datasets follow the operation descriptions.
For now all that is needed to know is that there are two datasets,
one of 34 thousand employers and one of 1.2 million parcels of land.

\subsubsection{Create}

The create operation adds a new record to an existing
dataset. Adding a record is the basic operation required to build a new
dataset. Record-centric systems merely add a record to the record store,
while dataset-centric systems output a new dataset that contains the data
of the input dataset with the new record included in it.  Expected output
is the original dataset with the addition of a single record.

The create operation implementations add a parcel of land to the parcels
dataset.

\subsubsection{Read}

The read operation extracts a single record from a dataset based on a
unique identifier.  Record-centric systems are able to employ indexes and
other methods to quickly locate a record whereas dataset-centric systems
must scan an entire dataset to produce the single record. Expected output
is a single record.

The read operation extracts a parcel from the parcels dataset.

\subsubsection{Update}

The update operation finds a single record from a dataset and changes
some attribute of the record.  Record-centric systems are able to employ
indexes to locate and modify the record. Dataset-centric systems generate
a new dataset with the changed record. Expected output is a dataset that
contains all the records from the input dataset with exception that the
specified record has been changed in the specified manner.

The update operation changes the land use code for a parcel in the
parcels dataset.

\subsubsection{Destroy}

The destroy operation finds and removes a record from a
dataset. Record-centric systems are able to find the record to be
removed and then removing it from the record store. Dataset-centric
systems generate a new dataset without the specified record. Expected
output is a dataset with all the records from the input dataset except
for the one that was removed.

The destroy operation removes a parcel from the parcels dataset.

\subsubsection{Filter}

The filter operation removes all records that don't fulfill a certain
requirement, in this case all the records that don't intersect with
a defined geometry. Record-centric systems do not have much of an
advantage here as indexing the results of some geospatial operation
is not usually worth the indexing cost, so both the record-centric and
dataset-centric systems must scan the entire dataset. Expected output
is a dataset containing all of the records from the input dataset that
fulfill the filtering requirements.

The filter operation removes all parcels of land that don't overlap a
defined region in the parcels dataset.

\subsubsection{Nearest}

The nearest operation is derived from an actual processing operation
required for the Digital Phoenix Project\cite{digitalphoenix}. This
operation uses two datasets. The first is a set of points representing
employers in the Phoenix metro-area. The second is a set of polygons
representing parcels of land in the same area. Digital Phoenix needed
to match the employer with the parcel of land where the business
should be. Thus the operation is for each employer, find the nearest
parcel of land with a compatible zoning code. The datasets used in this
thesis have been simplified to make this matching appear more straight
forward. Expected output is a list of employer ids with associated
parcel ids.

The nearest operation uses both the employers and parcels datasets.

\subsubsection{Chaining}

The chaining operation combines the filter and nearest operations with
the intent of showing how multiple operations can be performed one after
the other. This operation is an obvious optimization of the nearest
operation in that it first removes all residential parcels before running
the nearest operation on the remaining parcels. As employers cannot (in
this simplified world) exist on residential parcels, and since residential
parcels make up a large portion of the entire parcel dataset, the number
of distances calculated for each employer will be significantly decreased
thus decreasing processing time. Expected output is the same as for the
nearest operation.

The chaining operation uses both the employers and parcels datasets.

\subsection{Dataset Description}

Two datasets are used in evaluating these operations. The first is a
set of employers where each record contains an employer id, the place
where the employer is located represented as a point, and the business
classification: commercial, industrial, or governmental. The employer
dataset contains 34,302 records.

The second dataset is a set of parcels where each record contains a parcel
id, a multipolygon representing the parcel coverage, and a land use code:
residential, commercial, industrial, or governmental. The parcel dataset
contains 1,218,130 records.

Both datasets use R, C, I, and G to represent residential, commercial,
industrial, and governmental use codes. These datasets are simplified
versions of real datasets for Maricopa County, Arizona. The datasets
were simplified by adding the simplified land use code attribute and
removing the other attributes not used in these operations.

\section{Execution Environment}

All operations will be executed on ASU's Saguaro cluster. The Saguaro
cluster is comprised of several generations of hardware. To simplify
comparisons, all operations will be executed on similar hardware with
similar network interconnects, using the one most natural for use.

Saguaro is a CentOS 5.3 based cluster running on Intel Xeon
processors. Each node used in these experiments has two harpertown
processors each with four cores running at 2.83GHz and has 16GB of
RAM. In addition each node has one gigabit ethernet connection and one
DDR InfiniBand connection. The shared filesystem for these experiments
is Luster 1.6.6 accessed through the InifiniBand connection.

\section{PostGIS Implementation}

The PostGIS experiments were ran using PostgreSQL 8.4.1 and PostGIS
1.3. Both pieces of software were compiled for these tests and used
default options.

Each of the following operations was executed on a freshly setup PostGIS
instance, running on local disk with default options. Data was loaded,
indexed, and vacuumed before the experiment was executed. Runtimes were
gathered by using the linux `time' command. When a query's result did
not modify an existing table, its results were stored into another table.

Full SQL statements for each operation are listed below.

\subsection{Create}
\begin{lstlisting}[caption={PostGIS Create Operation},language=]
$ \addcontentsline{toc}{listchapter}{\lstname} $
insert into parcels values (97123897, ST_GeomFromText('POLYGON((-112.0859375 33.4349975585938,-112.0859375 33.4675445556641,-112.059799194336 33.4675445556641,-112.059799194336 33.4349975585938,-112.0859375 33.4349975585938))', 4326), 'C');
\end{lstlisting}

\subsection{Read}
\begin{lstlisting}[caption={PostGIS Read Operation},language=]
create table output as select * from parcels where id = 1008130;
\end{lstlisting}

\subsection{Update}
\begin{lstlisting}[caption={PostGIS Update Operation},language=]
update parcels set devtype = 'C' where id = 1008130;
\end{lstlisting}

\subsection{Destroy}
\begin{lstlisting}[caption={PostGIS Destroy Operation},language=]
delete from parcels where id = 1008130;
\end{lstlisting}

\subsection{Filter}
\begin{lstlisting}[caption={PostGIS Filter Operation},language=]
create table output as
select parcels.* from parcels, (select ST_GeomFromText('POLYGON((-112.0859375 33.4349975585938,-112.0859375 33.4675445556641,-112.059799194336 33.4675445556641,-112.059799194336 33.4349975585938,-112.0859375 33.4349975585938))', 4326) as the_geom) as box
where ST_Intersects(parcels.the_geom, box.the_geom);
\end{lstlisting}

\subsection{Nearest}
\begin{lstlisting}[caption={PostGIS Nearest Operation},language=]
create table output as
select employers.id as employer_id, parcels.id as parcel_id
from employers, parcels
where parcels.devtype = employers.devtype
and parcels.id in (
        select id from parcels where devtype = employers.devtype order by distance(employers.the_geom, the_geom) limit 1
);
\end{lstlisting}

\subsection{Chaining}
\begin{lstlisting}[caption={PostGIS Chained Operation},language=]
create table output as
select employers.id as employer_id, parcels.id as parcel_id
from employers, parcels
where employers.devtype <> 'R' and parcels.devtype = employers.devtype
and parcels.id in (
        select id from parcels where devtype = employers.devtype order by distance(employers.the_geom, the_geom) limit 1
);
\end{lstlisting}

\section{HadoopGIS Implementation}

The hadoopGIS experiments were executed using Hadoop 0.19.0, data was
accessed on an HDFS running on the same nodes off local disk. Internode
communication was accomplished through Gigabit Ethernet.

Default values were mostly used. Changes were made to increase the number
of mappers per node to 8, increasing the memory available to each JVM,
and altering the HDFS block size to produce the desired number of mappers.

Core algorithm details are described below. In all cases the reduce
function is an identity function where all input is passed through as
output. Full listings are available in the appendix.

\subsection{Create}

The Hadoop architecture makes it difficult to arbitrarily output
a record. The processing method is fully dependent on the input
records. Since it was not possible to add a record to the output
dataset without relying on some input, like is possible for PostGIS and
clusterGIS, this operation creates the new record when triggered by a
record that already exists in the dataset. The resulting map function
is as follows:

\begin{lstlisting}[caption={HadoopGIS Create Operation},language=]
if(key == 1008130) {
        GIS created = new GIS();
        GIS.update("\"97123897\",\"POLYGON((-112.0859375 33.4349975585938,-112.0859375 33.4675445556641,-112.059799194336 33.4675445556641,-112.059799194336 33.4349975585938,-112.0859375 33.4349975585938))\",\"C\"");
        output.collect(new LongWritable(new Integer(97123897)), created);
}
output.collect(key, value);
\end{lstlisting}

\subsection{Read}
\begin{lstlisting}[caption={HadoopGIS Read Operation},language=]
if(key.equals(new LongWritable(1008130))) {
        output.collect(key, value);
}
\end{lstlisting}

\subsection{Update}
\begin{lstlisting}[caption={HadoopGIS Update Operation},language=]
if(key.equals(new LongWritable(1008130))) {
        value.attributes.put("devtype", "C");
}
output.collect(key, value);
\end{lstlisting}

\subsection{Destroy}
\begin{lstlisting}[caption={HadoopGIS Destroy Operation},language=]
if(!key.equals(new LongWritable(1008130))) {
        output.collect(key, value);
}
\end{lstlisting}

\subsection{Filter}

To keep from recreating the geometry filtered against, the geometry is
created in the configure method which is ran at the creation of the
mapper. It is made available as a Geometry object named box and used
as follows:

\begin{lstlisting}[caption={HadoopGIS Filter Operation},language=]
if(value.geometry.intersects(box)) {
        output.collect(key, value);
}
\end{lstlisting}

\subsection{Nearest}

The nearest operation uses the employers dataset as input for the map
functions. As the operations compares every employer against every parcel,
the parcel dataset is loaded in its entirety in the configure method
(ran when creating each mapper). This way of using a secondary dataset
is not optimal, but it is the only method allowed in the map-reduce
framework. Being forced to include a full copy of the parcels dataset
increases the memory requirement for each mapper, thus limited the number
of mappers that can be ran on a single machine and the maximum size of
the parcel dataset. The clusterGIS implementation presents a solution
for this problem that is possible in MPI but not Hadoop.

Even with that problem, the resulting map function is quite simple:

\begin{lstlisting}[caption={HadoopGIS Nearest Operation},language=]
double minDistance = Double.MAX_VALUE, currDistance;
int closestParcel = -1;

Iterator it = parcels.iterator();
while (it.hasNext())
{
        GIS parcel = (GIS) it.next();

        currDistance = value.geometry.distance(parcel.geometry);
        if (currDistance < minDistance)
        {
                minDistance = currDistance;
                closestParcel = new Integer(parcel.attributes.get("id"));
        }
}

LongWritable lngClosestParcel = new LongWritable (closestParcel);
output.collect(key, lngClosestParcel);
\end{lstlisting}

\subsection{Chaining}

The chaining operation reduces the memory problems encountered in the
nearest operation implementation by removing all residential parcels. This
optimization works for this specific problem but cannot be generally
applied. As the parcel dataset loading is done in the configure method,
there is no change to the map function:

\begin{lstlisting}[caption={HadoopGIS Chained Operation},language=]
double minDistance = Double.MAX_VALUE, currDistance;
int closestParcel = -1;

Iterator it = parcels.iterator();
while (it.hasNext())
{
        GIS parcel = (GIS) it.next();

        currDistance = value.geometry.distance(parcel.geometry);
        if (currDistance < minDistance)
        {
                minDistance = currDistance;
                closestParcel = new Integer(parcel.attributes.get("id"));
        }
}

LongWritable lngClosestParcel = new LongWritable (closestParcel);
output.collect(key, lngClosestParcel);
\end{lstlisting}

\section{ClusterGIS Implementation}
\lstset{language=C}

ClusterGIS was compiled with the Intel C Compiler (icc) 10.1 20080312
against MVAPICH 1.0.1.  MVAPICH makes use of the DDR InfiniBand network
connection for communication. Dataset storage is done on Lustre, which
is connected through the same InifiBand connections.

The specific main portion of each operations algorithm is show
below. Dataset loading and writing is accomplished using the distributed
method including all tasks in the operation unless otherwise stated. Full
code listings are available in the appendix.

\subsection{Create}

Unlike the hadoopGIS create operation, it is easy to arbitrarily add
a record using clusterGIS. In this case the first tasks creates a new
record using a helper function provided by clusterGIS and then inserts
it into its local dataset. When the dataset is written collectively,
the new record is included.

\begin{lstlisting}[caption={ClusterGIS Create Operation},language=]
if(rank == 0) {
        int start = 0;
        record = clusterGIS_Create_record_from_csv("97123897,POINT(0 0),C\n", &start);
        record->next = dataset->data;
        dataset->data = record;
}
\end{lstlisting}

\subsection{Read}

The hardest part of the read operation is removing records from the
dataset. This example shows how to accomplish this.

\begin{lstlisting}[caption={ClusterGIS Read Operation},language=]
record = dataset->data;
head = &(dataset->data);
/* keep records that match the criteria, otherwise delete them */
while(record != NULL) {
        if(atoi(record->data[0]) == 1008130) {
                head = &(record->next);
                record = record->next;
        } else {
                *head = record->next;
                clusterGIS_Free_record(record);
                record = *head;
        }
}
\end{lstlisting}

\subsection{Update}
\begin{lstlisting}[caption={ClusterGIS Update Operation},language=]
record = dataset->data;
/* keep records that match the criteria, otherwise delete them */
while(record != NULL) {
        if(atoi(record->data[0]) == 1008130) {
                record->data[2] = "C";
        }
        record = record->next;
}
\end{lstlisting}

\subsection{Destroy}
\begin{lstlisting}[caption={ClusterGIS Destroy Operation},language=]
record = dataset->data;
head = &(dataset->data);
/* keep records that match the criteria, otherwise delete them */
while(record != NULL) {
        if(atoi(record->data[0]) == 1008130) {
                *head = record->next;
                clusterGIS_Free_record(record);
                record = *head;
        } else {
                head = &(record->next);
                record = record->next;
        }
}
\end{lstlisting}

\subsection{Filter}
\begin{lstlisting}[caption={ClusterGIS Filter Operation},language=]
box = GEOSWKTReader_read(reader, "POLYGON((-112.0859375 33.4349975585938,-112.0859375 33.4675445556641,-112.059799194336 33.4675445556641,-112.059799194336 33.4349975585938,-112.0859375 33.4349975585938))");

record = dataset->data;
head = &(dataset->data);
/* keep records that match the criteria, otherwise delete them */
while(record != NULL) {
        char intersects;

        /*record_geometry = GEOSWKTReader_read(reader, record->data[1]);
        if(record_geometry == NULL) {
                fprintf(stderr, "%d: parse error\n", rank);
                MPI_Abort(MPI_COMM_WORLD, 2);
        }*/

        intersects = GEOSIntersects(record->geometry, box);
        if(intersects == 2) {
                fprintf(stderr, "%d: error with overlap function\n", rank);
                MPI_Abort(MPI_COMM_WORLD, 2);
        }

        if(intersects == 1) { /* record overlaps with box */
                head = &(record->next);
                record = record->next;
        } else { /* no overlap */
                *head = record->next;
                clusterGIS_Free_record(record);
                record = *head;
        }
}
\end{lstlisting}

\subsection{Nearest}

Upto this point, all the operations for clusterGIS have only had
to work on one dataset. The nearest and chained operations work on
two. Unlike hadoopGIS, clusterGIS is able to split up both datasets
between participating tasks. The tested implementation places a full copy
of the parcels dataset on each computer, which has eight tasks running on
it, where each of those tasks has the same set of employers. This means
that each task will find the closest parcel for each employer of the
parcels that it has (i.e., a local minimum). The tasks sharing the same
employers then talk amongst themselves to determine the globally nearest
parcel. This communication is done through MPI's reduce mechanism. The
employer id now associated parcel id are added to the output dataset.

\begin{lstlisting}[caption={ClusterGIS Nearest Operation},language=]
/* Find the min distance */
employer = employers->data;
min = malloc(sizeof(double)*2);
global_min = malloc(sizeof(double)*2);
output_csv = malloc(sizeof(char)*128);
output = clusterGIS_Create_dataset();
while(employer != NULL) {

        /* find the local min */
        parcel = parcels->data;
        GEOSDistance(employer->geometry, parcel->geometry, &min_distance);
        min_distance_parcel = parcel;
        while(parcel != NULL) {
                if(strncmp(employer->data[2], parcel->data[2], 1) == 0) {
                        GEOSDistance(employer->geometry, parcel->geometry, &distance);
                        if(distance < min_distance) {
                                min_distance = distance;
                                min_distance_parcel = parcel;
                        }
                }
                parcel = parcel->next;
        }

        /* find the global min */
        min[0] = atoi(min_distance_parcel->data[0]);
        min[1] = min_distance;
        MPI_Allreduce(min, global_min, 2, MPI_DOUBLE, min_distance_op, parcels_comm);

        /* Add the min to the output dataset */
        sprintf(output_csv, "\"%s\",\"%d\"\n", employer->data[0], (int) global_min[0]);
        start = 0;
        output_record = clusterGIS_Create_record_from_csv(output_csv, &start);
        output_record->next = output->data;
        output->data = output_record;

        employer = employer->next;
}
\end{lstlisting}

\subsection{Chaining}

The chaining operation works in a similar manner to the nearest operation
except that it filters out all residential parcels before searching for
the nearest parcel.

\begin{lstlisting}[caption={ClusterGIS Chaining Operation},language=]
/* remove all residential parcels */
parcel = parcels->data;
head = &(parcels->data);
while(parcel != NULL) {
        if(strncmp(parcel->data[2], "R", 1) == 0) {
                head = &(parcel->next);
                parcel = parcel->next;
        } else {
                *head = parcel->next;
                clusterGIS_Free_record(parcel);
                parcel = *head;
        }
}

/* Find the min distance */
employer = employers->data;
min = malloc(sizeof(double)*2);
global_min = malloc(sizeof(double)*2);
output_csv = malloc(sizeof(char)*128);
output = clusterGIS_Create_dataset();
while(employer != NULL) {

        /* find the local min */
        parcel = parcels->data;
        GEOSDistance(employer->geometry, parcel->geometry, &min_distance);
        min_distance_parcel = parcel;
        while(parcel != NULL) {
                if(strncmp(employer->data[2], parcel->data[2], 1) == 0) {
                        GEOSDistance(employer->geometry, parcel->geometry, &distance);
                        if(distance < min_distance) {
                                min_distance = distance;
                                min_distance_parcel = parcel;
                        }
                }
                parcel = parcel->next;
        }

        /* find the global min */
        min[0] = atoi(min_distance_parcel->data[0]);
        min[1] = min_distance;
        MPI_Allreduce(min, global_min, 2, MPI_DOUBLE, min_distance_op, parcels_comm);

        /* Add the min to the output dataset using front insertion*/
        sprintf(output_csv, "\"%s\",\"%d\"\n", employer->data[0], (int) global_min[0]);
        start = 0;
        output_record = clusterGIS_Create_record_from_csv(output_csv, &start);
        output_record->next = output->data;
        output->data = output_record;

        employer = employer->next;
}
\end{lstlisting}
