\chapter{EXPERIMENTAL SETUP AND RESULTS}

Chapter 3 defined the requirements for a good parallel GIS processing
engine. Chapter 4 discussed the designs used in hadoopGIS and
clusterGIS. Given these requirements and designs, the implementations
can now be evaluated and compared against each other. As both these
implementations are experimental, PostGIS will be used as a reference
point to represent current methods. As such, it will also be evaluated
as much as possible in the same manner as hadoopGIS and clusterGIS.

\section{Standard Geospatial Operations}

The first requirement is that an implementation can perform a set of
standard geospatial operations. Allowing full access to a compliant
geospatial library fulfills this requirement a priori. PostGIS uses,
and in fact developed, the GEOS library. HadoopGIS makes use of JTS,
from which the GEOS library was ported. ClusterGIS uses the GEOS library.

All three implementations fulfill this requirement, no further
experimentation is needed.

\section{Batch Mode Processing}

Batch mode operation is essential to running operations on a cluster.

PostGIS extends the PostgreSQL database. PostgreSQL can be accessed
through many different methods including programming interfaces in
C and other languages, or through the provided command line client,
``psql''. This client can be used interactively, or a sql script can be
passed to it. That leaves the problem of the server. Either a dedicated
server can be used, or a script can be written to create a server as
needed to do the processing. This thesis sets a PostgreSQL server up as
needed. PostGIS can perform batch mode processing.

Hadoop executes user jobs from a command line interface. The only
hindrance for Hadoop to batch mode processing is the same as PostGIS:
Hadoop is designed to have a persistent server environment setup. Projects
like Hadoop on Demand allow a Hadoop environment to be created as
needed. This thesis accomplishes a similar solution through a custom
designed script.  With this setup, HadoopGIS is capable of batch mode
processing.

ClusterGIS only operates in a batch mode, there is no interactive mode.

\section{Scalability}

The first two requirements did not require any experimentation to
evaluate. Scalability requires experimentation. This section details the
experiments and specific implementation details. Chapter 6 presents and
discusses the results of these experiments.

Verification of operations will be done by comparing output of each
operation for HadoopGIS and ClusterGIS with the output from a PostGIS
reference implementation.

As HadoopGIS and ClusterGIS are processing engines, scalability depends
on the individual processing that is done. To achieve this, a set of
processing operations is defined. After the operations are discussed in
general, specific details for each implementation are discussed.

\subsection{Operation set}

\begin{table}
\begin{center}
\begin{tabular}{lcc}
Section & 1 Geometry & 2 Geometries\\
\hline
6.1.2 (Geometry) & 16 & 15\\
6.1.4 (Point) & 4 & 0\\
6.1.6 (Curve) & 5 & 0\\
6.1.7 (LineString, Line, LinearRing)& 2 & 0\\
6.1.8 (MultiCurve) & 2 & 0\\
6.1.10 (Surface) & 3 & 0\\
6.1.11 (Polygon, Triangle) & 3 & 0\\
6.1.12 (PolyhedralSurface) & 4 & 0\\
6.1.13 (MultiSurface) & 3 & 0\\
6.1.15 (Relational Operators) & 0 & 9\\
\hline
Total & 42 & 24\\
\end{tabular}
\caption{OGC Methods by Number of Required Geometries}
\label{MethodCount}
\end{center}
\end{table}

One of the main tasks in parallelizing an algorithm is solving the
problem of data access. HadoopGIS and clusterGIS split up the data that
is being processed between different computers. The OGC SFS standard
defines the operations that need to be supported. Table \ref{MethodCount}
shows a count of the number of operations in each section of the standard
that defines operations along with the number of geometries required to
execute that operation. One geometry is required for 42 of the operations,
while 24 operations require two. There are no operations that require
more than two geometries.

The problem faced by HadoopGIS and ClusterGIS then is to get the data
required to perform the geospatial operations required by what ever
processing is required. The operations defined here are meant to be
representative of what actual processing requirements would demand.
The first four operations deal with individual record access. The
following three operations deal with getting the data needed for a
geospatial operation where it is needed.

Specific details on the datasets follow the operation descriptions.
For now all that is needed to know is that there are two datasets,
one of 34 thousand employers and one of 1.2 million parcels of land.

\subsubsection{Create}

The create operation adds a new record to an existing
dataset. Adding a record is the basic operation required to build a new
dataset. Record-centric systems merely add a record to the record store,
while dataset-centric systems output a new dataset that contains the data
of the input dataset with the new record included in it.  Expected output
is the original dataset with the addition of a single record.

The create operation implementations add a parcel of land to the parcels
dataset.

\subsubsection{Read}

The read operation extracts a single record from a dataset based on a
unique identifier.  Record-centric systems are able to employ indexes and
other methods to quickly locate a record whereas dataset-centric systems
must scan an entire dataset to produce the single record. Expected output
is a single record.

The read operation extracts a parcel from the parcels dataset.

\subsubsection{Update}

The update operation finds a single record from a dataset and changes
some attribute of the record.  Record-centric systems are able to employ
indexes to locate and modify the record. Dataset-centric systems generate
a new dataset with the changed record. Expected output is a dataset that
contains all the records from the input dataset with exception that the
specified record has been changed in the specified manner.

The update operation changes the land use code for a parcel in the
parcels dataset.

\subsubsection{Destroy}

The destroy operation finds and removes a record from a
dataset. Record-centric systems are able to find the record to be
removed and then removing it from the record store. Dataset-centric
systems generate a new dataset without the specified record. Expected
output is a dataset with all the records from the input dataset except
for the one that was removed.

The destroy operation removes a parcel from the parcels dataset.

\subsubsection{Filter}

The filter operation removes all records that don't fulfill a certain
requirement, in this case all the records that don't intersect with
a defined geometry. Record-centric systems do not have much of an
advantage here as indexing the results of some geospatial operation
is not usually worth the indexing cost, so both the record-centric and
dataset-centric systems must scan the entire dataset. Expected output
is a dataset containing all of the records from the input dataset that
fulfill the filtering requirements.

The filter operation removes all parcels of land that don't overlap a
defined region in the parcels dataset.

\subsubsection{Nearest}

The nearest operation is derived from an actual processing operation
required for the Digital Phoenix Project\cite{digitalphoenix}. This
operation uses two datasets. The first is a set of points representing
employers in the Phoenix metro-area. The second is a set of polygons
representing parcels of land in the same area. Digital Phoenix needed
to match the employer with the parcel of land where the business
should be. Thus the operation is for each employer, find the nearest
parcel of land with a compatible zoning code. The datasets used in this
thesis have been simplified to make this matching appear more straight
forward. Expected output is a list of employer ids with associated
parcel ids.

The nearest operation uses both the employers and parcels datasets.

\subsubsection{Chaining}

The chaining operation combines the filter and nearest operations with
the intent of showing how multiple operations can be performed one after
the other. This operation is an obvious optimization of the nearest
operation in that it first removes all residential parcels before running
the nearest operation on the remaining parcels. As employers cannot (in
this simplified world) exist on residential parcels, and since residential
parcels make up a large portion of the entire parcel dataset, the number
of distances calculated for each employer will be significantly decreased
thus decreasing processing time. Expected output is the same as for the
nearest operation.

The chaining operation uses both the employers and parcels datasets.

\subsection{Dataset Description}

Two datasets are used in evaluating these operations. The first is a
set of employers where each record contains an employer id, the place
where the employer is located represented as a point, and the business
classification: commercial, industrial, or governmental. The employer
dataset contains 34,302 records.

The second dataset is a set of parcels where each record contains a parcel
id, a multipolygon representing the parcel coverage, and a land use code:
residential, commercial, industrial, or governmental. The parcel dataset
contains 1,218,130 records.

Both datasets use R, C, I, and G to represent residential, commercial,
industrial, and governmental use codes. These datasets are simplified
versions of real datasets for Maricopa County, Arizona. The datasets
were simplified by adding the simplified land use code attribute and
removing the other attributes not used in these operations.

\section{Execution Environment}

All operations will be executed on ASU's Saguaro cluster. The Saguaro cluster is comprised of several
generations of hardware. To simplify comparisons, all operations will be executed on similar hardware
with similar network interconnects, using the one most natural for use.

Saguaro is a CentOS 5.3 based cluster running on Intel Xeon processors. Each node used in these experiments
has two harpertown processors each with four cores running at 2.83GHz and has 16GB of RAM. In addition each node has one gigabit ethernet connection and one DDR InfiniBand connection. The shared filesystem for these experiments is Luster 1.6.6 accessed through the InifiniBand connection.

\section{PostGIS Implementation}

The PostGIS experiments were ran using PostgreSQL 8.4.1 and PostGIS 1.3. Both pieces of software were compiled for these tests and used default options.

Each of the following operations was executed on a freshly setup PostGIS instance, running on local disk with default options. Data was loaded, indexed, and vacuumed before the experiment was executed. Runtimes were gathered by using the linux `time' command.

Full SQL statements for each operation are listed below.

% FIXME: PostGIS operations

\subsection{Create}

insert...

\subsection{Read}

select ... where id =

\subsection{Update}

update....

\subsection{Destroy}

delete ...

\subsection{Filter}

select...where

\subsection{Nearest}

Crazy set of queries (nested would have been nice, but too slow)

\subsection{Chaining}

Modify crazy set of queries to use the filter

\section{HadoopGIS Implementation}

The hadoopGIS experiments were executed using Hadoop 0.19.0, data was accessed on an HDFS running on the same nodes off local disk. Internode communication was accomplished through Gigabit Ethernet.

Default values were mostly used. Changes were made to increase the number of mappers per node to 8, increasing the memory available to each JVM, and altering the HDFS block size to produce the desired number of mappers.

Core algorithm details are described below. Full listings are available in the appendix.

% FIXME: HadoopGIS Operation stubbs and explanations

\subsection{Create}

map: emit all record giving them the same key (forcing one reduce); reduce emits all received records with the addition of one

\subsection{Read}

map: emit only the record with the correct id; reduce: identity

\subsection{Update}

map: emit all records, updating the one with the correct id; reduce: identity

\subsection{Destroy}

map: emit all records except for the one with the correct id; reduce: identity

\subsection{Filter}

map: emit only records matching the requested criteria; reduce: identity

\subsection{Nearest}

map: loop through secondary dataset emitting id of primary and secondary; reduce: identity

\subsection{Chaining}

(Filter the datasets, then find the nearest in the filtered data)

filter secondary dataset on load; map: only search for nearest for filtered data, skip others; reduce: identity

\section{ClusterGIS Implementation}
\lstset{language=C}

ClusterGIS was compiled with the Intel C Compiler (icc) 10.1 20080312
against MVAPICH 1.0.1.  MVAPICH makes use of the DDR InfiniBand network
connection for communication. Dataset storage is done on Lustre, which
is connected through the same InifiBand connections.

The specific implementation of each operation is discussed below. The
main block of code is included and discussed. Full code listings are
available in the appendix.


% FIXME: clusterGIS operations stubbs and explanations

\subsection{Create}

The create operation is quite simple. First the dataset is loaded in a
distributed manner across all the tasks included in the operation. One
task then adds a record after which the dataset is written to disk.

The record addition code is as follows:

\begin{lstlisting}[firstnumber=1]
record = clusterGIS_Create_record_from_csv("97123897,POINT(0 0),C\n",
	&start);
record->next = dataset->data;
dataset->data = record;
\end{lstlisting}

Lines 1-2 create the record using the clusterGIS\_Create\_record\_from\_csv 

\subsection{Read}

emit only the record with the correct id;

\subsection{Update}

emit all records updating the correct one

\subsection{Destroy}

emit all record except the correct one

\subsection{Filter}

emit only records matching the requested criteria

\subsection{Nearest}

loop through primary and secondary dataset, emitting the id of primary and secondary which match

\subsection{Chaining}

(Filter the datasets, then find the nearest in the filtered data)

remove nodes in the datasets not matching the filter, find nearest as before
