\chapter{EXPERIMENTAL SETUP}

\textbf{How to show how well requirements were filled}

Some of the needed research has already been completed. After determining
the query set and creating the PostGIS implementation of it, the
processing environment was prototyped in Hadoop\cite{hadoop}, an opensource
implementation of Google's MapReduce\cite{mapreduce} framework. MapReduce works
by utilizing two user defined function, map and reduce. For each record
of the primary dataset the map function is called. This process is made
parallel by splitting the data up among several computers and running
the individual map functions on those computers. After the map function
is complete, the resulting set of data is passed to several reduce
functions. Each instance of the reduce function receives all the data from
the resulting dataset associated with the same key. The lessons learned
from this prototyping phase will be applied to the MPI implementation.

\section{Standard Geospatial Operations}

Used JTS, GEOS.

\section{Batch Mode Processing}

Method of execution

\section{Scalability}

\subsection{Operation set}

Assuming OGC-SFS compliance (Because of using JTS and GEOS),
operations come down to data interaction. How to get data where
it belongs. Differences between record-centric and dataset-centric
processing methods.

http://www.opengeospatial.org/standards/sfa (Simple Feature Access - Part 1: Common Architecture 1.2.0)

\begin{table}
\begin{center}
\begin{tabular}{lcc}
Section & 1 Geometry & 2 Geometries\\
\hline
6.1.2 (Geometry) & 16 & 15\\
6.1.4 (Point) & 4 & 0\\
6.1.6 (Curve) & 5 & 0\\
6.1.7 (LineString, Line, LinearRing)& 2 & 0\\
6.1.8 (MultiCurve) & 2 & 0\\
6.1.10 (Surface) & 3 & 0\\
6.1.11 (Polygon, Triangle) & 3 & 0\\
6.1.12 (PolyhedralSurface) & 4 & 0\\
6.1.13 (MultiSurface) & 3 & 0\\
6.1.15 (Relational Operators) & 0 & 9\\
\hline
Total & 42 & 24\\
\end{tabular}
\caption{OGC Methods by Number of Required Geometries}
\label{MethodCount}
\end{center}
\end{table}

All operations must be performed as if they were part of a series of operations.

\subsubsection{Create}

Adds a new record to an existing dataset, serial id needs to be calculated.

Compares record-centric to dataset-centric. What gets written? Mutability of dataset?

\subsubsection{Read}

Reads an existing record (by id)

Compares record-centric to dataset-centric access times.

\subsubsection{Update}

Updates an attribute of a record

Compares record-centric to dataset-centric. What gets written? Mutability of dataset?

\subsubsection{Destroy}

Removes a record from the dataset.

Compares record-centric to dataset-centric. What gets written? Mutability of dataset?

\subsubsection{Filter}

Apply a geospatial operation across the entire dataset - should be better for dataset-centric methods.

\subsubsection{Nearest}

Find the nearest feature in a secondary dataset for every feature in the primary dataset.

\subsubsection{Chaining}

Filter the datasets, then find the nearest in the filtered data

Can we reuse data without pushing it to disk?

\subsection{Dataset Description}

Full datasets; 34k employers, 1.2m parcels

sub datasets, how to generate from full

Operations 1-5:Use parcels 

1 record
10 records
100 records
1,000 records
10,000 records
100,000 records
1,000,000 records
1.2M records (full dataset)


Operations 6-7: Employers and Parcels

1 sq mile
10 sq mile
100 sq mile
...
full

\subsection{Required Runs}

\section{Execution Environment}

Description of Saguaro (common execute environment)

CentOS 5.3 based

Moab/Torque scheduler

GigE/Inifiband network

Lustre version, options

\section{PostGIS Implementation}

PostgreSQL 8.4

PostGIS 1.3....

Full or chunk of code per operation, description of how the operation was accomplished

All operations must be performed as if they were part of a series of operations.

\subsection{Create}

insert...

\subsection{Read}

select ... where id =

\subsection{Update}

update....

\subsection{Destroy}

delete ...

\subsection{Filter}

select...where

\subsection{Nearest}

Crazy set of queries (nested would have been nice, but too slow)

\subsection{Chaining}

Modify crazy set of queries to use the filter

\section{HadoopGIS Implementation}

Hadoop 0.20.0

HOD on Saguaro

JTS ...

JDK ...

Full or chunk of code per operation, description of how the operation was accomplished

All operations must be performed as if they were part of a series of operations.

\subsection{Create}

map: emit all record giving them the same key (forcing one reduce); reduce emits all received records with the addition of one

\subsection{Read}

map: emit only the record with the correct id; reduce: identity

\subsection{Update}

map: emit all records, updating the one with the correct id; reduce: identity

\subsection{Destroy}

map: emit all records except for the one with the correct id; reduce: identity

\subsection{Filter}

map: emit only records matching the requested criteria; reduce: identity

\subsection{Nearest}

map: loop through secondary dataset emitting id of primary and secondary; reduce: identity

\subsection{Chaining}

(Filter the datasets, then find the nearest in the filtered data)

filter secondary dataset on load; map: only search for nearest for filtered data, skip others; reduce: identity

\section{ClusterGIS}

Intel compiler 10....

MVAPICH 1.0.1

IB backend

Lustre

Full or chunk of code per operation, description of how the operation was accomplished

All operations must be performed as if they were part of a series of operations.

\subsection{Create}

emits all records and the new one

\subsection{Read}

emit only the record with the correct id;

\subsection{Update}

emit all records updating the correct one

\subsection{Destroy}

emit all record except the correct one

\subsection{Filter}

emit only records matching the requested criteria

\subsection{Nearest}

loop through primary and secondary dataset, emitting the id of primary and secondary which match

\subsection{Chaining}

(Filter the datasets, then find the nearest in the filtered data)

remove nodes in the datasets not matching the filter, find nearest as before
