\chapter{RELATED WORK}

This thesis applies parallel processing techniques to the field of
GIS processing. I will first look at the state of GIS processing, then
Parallel Processing in general, then parallel GIS processing.

\section{GIS Processing}

Geographic Information Systems (GIS)\cite{gis} have been in use since the
1960's with the Canada Geographic Information System (CGIS)\cite{cgis}
and then moving from the mainframes to current desktop applications like
ESRI's ArcGIS\cite{arcgis} product, which began development in the 1980's.

In general, there are two types of GIS data: raster and vector. Raster
data is a set of cells, such as pixels in a picture, that have one or
more attributes (e.g., temperature, humidity, elevation). Each attribute
covers the entire area of the pixel. The entire raster dataset is
spatially located and states how much area is covered by each cell.

Vector data is comprised of spatially referenced geometric objects such
as points, lines, and polygons. Each object represents something in the
real world, and has associated attributes.

Most GIS processing systems are able to handle both raster and vector
data sources.

\subsection{Desktop GIS}

Desktop GIS packages such as ArcGIS\cite{arcgis}, QuantumGIS\cite{qgis},
and GRASS GIS\cite{grass} are commonly used for GIS processing and
analysis. While these programs provide graphical interfaces to their GIS
capabilities, their capabilities are limited by the computers they run
on. Datasets can be too large for their memories and computations can
take too long to be practical.  

Desktop packages are often supplemented with a database component such as
ArcSDE\cite{arcsde} or PostGIS\cite{postgis} which acts as a centralized
repository for GIS data which can be shared between a workgroup. It is
important to note that the database is generally used for storage and
sharing, not for computation. Computation is performed by the desktop
package.

\subsection{Database GIS}

An alternative to performing GIS processing in a desktop program like
ArcGIS, is to employ a geospatial database like PostGIS\cite{postgis},
ArcSDE\cite{arcsde}, or Oracle Spatial\cite{oracle}. Geospatial databases
allow centralized access to, and processing of, geospatial data through
query languages such as SQL. As data is stored and managed by the database
software, advanced database features such as indexes can be utilized to
speedup data access and processing.

PostGIS is utilized as the core component of the Urban Systems
Frame-work\cite{usf} (USF) designed by the Digital
Phoenix\cite{digitalphoenix} project group at Arizona State University.
Digital Phoenix tries to integrate 3D visualization technology with simulated
and gathered GIS data to better understand the impacts of urban planning
decisions.

\subsection{GIS Simulation and Analysis}

GIS simulation and analysis can also be done using more specialized
environments. UrbanSim\cite{urbansim} is a popular simulation tool for
growth models.

GeoDa\cite{geoda} is a spatial analysis tool that includes spatial
regressions. PySal\cite{pysal} is a python library that builds on the
work done with GeoDa.

These tools provide additional capabilities for GIS processing, but are
limited to working on a single computer. As the data becomes larger and
the analysis more complex, they become unable to perform the required
processing in a reasonable time, if at all.

\subsection{GIS Libraries}

The Open Geospatial Consortium (OGC) defined a core set of geospatial
processing operations in their Simple Features\cite{ogc-sfs}
standard. These core operations allow for most geospatial processing
needs. One of the main libraries that provides these operations and
related data types is the Java Topology Suite\cite{jts} (JTS). Though
written in Java, the JTS has been used as the basis for ports into other
languages. The Geometry Engine, Open Source (GEOS) library is a C++
port of the JTS that also provides a C interface. PostGIS is implemented
using the GEOS library. The NetTopologySuite\cite{nts} (NTS) is a port
of the JTS into .NET.

The Simple Features standard and the libraries that support it have
distilled the most important functions for GIS processing. The standard
through the experience and discussion employed in the creation process,
and the libraries by practical use in other projects and environments.

As quality libraries are available for a variety of languages, there
is no need to re-implement the functionality they provide. This thesis
makes direct use of the JTS and GEOS libraries.

\section{Parallel Processing}

\subsection{Serial and Parallel Shared-Memory Applications}

Computer programs are executed by processors. The simplest of programs
is made up of a series of operations that are executed in order by
the processor. As this type of application is comprised by a series
of operations it is known as a serial program. Serial programs are not
able to take advantage of more than one processor. To utilize more than
one processor at a time, a set of serial programs that can communicate
with each other is required. Each serial program in this set is referred
to as a thread. Thus multi-threaded programs can utilize more than one
processing core. The simplest method of communication between threads is
to share a common section of memory. Therefore a parallel shared-memory
application could utilize at most the number of processors able to be
connected to a single section of memory.

Both serial and parallel shared-memory applications are limited to
a single computer, where computer means a group of processors that
are connected to the same memory. Most modern computers provide this
capability.

\subsection{Parallel Distributed-Memory Applications}

One method to overcome the limitations of a single computer is to use
multiple computers. By providing a way for the computers to communicate,
and therefore collaborate, with each other. By means of collaboration,
the individual computers are able to act together to solve a single
problem. While the participating computers can communicate, they do not
have direct access to each other's memory. Thus the total memory available
to the collection of processors is distributed between them, not shared.

Because each computer only has direct access to its own memory, meaning
that finding out what is in the memory of another computer is harder,
the main task in designing a parallel program is figuring out how to
split up the work between computers. One method is to split up the processing
between computers. Each task would generally have the same data and perform
variations of an operation on the data; for instance, running multiple
scenarios. This methodology is called Task Parallel. An example of task
parallelism is simulating the effects of various weather patterns on
an urban environment. Each task would have a copy of the environment
and run its variety of weather on it. The capability of executing each
scenario is limited to the capabilities of a single processing element,
but many scenarios can be executed at the same time.

Data Parallel methods split the data up between computers and perform the
same, or similar, operations on each piece of data. To calculate the
effects of a weather pattern on an urban environment, the environment
would first be divided between the tasks with each task responsible for
one part of the environment. Each task would then calculate the effects
of the weather on its section of the environment, communicating with the
other computers as needed to share information related to edge conditions, etc.

\subsection{Message Passing Interface}

The Message Passing Interface\cite{mpi} (MPI) standard has become the normal way
of programming parallel distributed-memory applications. MPI works
by starting processing tasks on each of the computers alloted to the MPI
process. These tasks are then able to send and receive messages between
themselves allowing for inter-task communication.

As MPI defines a simple paradigm for inter-task communication,
any parallelism in the application must be explicitly specified and
programmed. Thus MPI programming is not simple or easy, though it is
not without benefits.

MPI libraries are able to utilize advanced network layers such as
InfiniBand without changes to the application, except for recompilation
against the library. This design allows for advances in technology to
be passed onto parallel application with little effort.

MPI is also able to take advantage of parallel filesystems such as
Lustre\cite{lustre}.  Parallel filesystems spread file data between
several file servers. Client programs are then able to access each part
of the file in parallel, speeding file reading and writing while enabling
each task to read or write a portion of data while not conflicting with
the other tasks in the same MPI process.

MPI is generally deployed on clusters, making programs that are based
on MPI able to run on a variety of machines.

\subsection{Map Reduce}

Map reduce is a functional programming idiom that has recently
become popular due to Google's extensive use first described in a
2004 paper\cite{mapreduce}. While Google's MapReduce environment is
proprietary, the concepts and idea have passed into the open source
Hadoop\cite{hadoop} framework developed by the Apache Software
Foundatation with major support by Yahoo! and
Cloudera.

Map reduce is relatively simple to program for. The programmer only
needs to supply two functions: map and reduce. The map function takes
as its input a record of the input data, record splitting is handled
by the framework, and outputs zero or more key-value pairs. The reduce
function takes a key and a set of values associated with that key and
outputs zero or more key-value pairs.

Unlike MPI, parallelism is handled by the map-reduce framework which
loads a portion of the entire input dataset on each of the machines
participating processing elements, splits the data into records,
executes the map function on each record, aggregates all the key-value
pairs produced by the map functions and runs the reduce function on each
unique key, passing the associated values along. At last the output from
the mappers is collected into the final output of the program. Parallel
operations are done implicitly, and therefore do not need to be created
by the programmer.

Map reduce was designed on the assumptions that disk is cheap, networking
is expensive, and that large systems can be built with inexpensive
hardware, as long as no piece of hardware in indispensable. With these
restrictions map reduce makes use of a distributed file system that
replicates data between several of these inexpensive computers. Map reduce
tasks are then ran on one of the computers that has the block of data it
needs. The blocking used to spread files on the distributed file system
are also used as the basis for parallelizing the map reduce process.

Map reduce is basically a two phase solution for parallel computing. The
first phase applies a function, map, to each record in the input
dataset. This phase is inherently parallel as each call to map is
completely independent, requiring no more data than just the record that
only it is responsible for. The second phase, reduce, is inherently
not parallel. Some parallelization is given by Hadoop in this phase
by limiting a reduce function from seeing all the data produced by the
map function to only the data produced with the same key (each output
being in the form of a key-value pair). Because of this limitation,
reduce has some parallelism, but much much less than in map.

Hadoop itself is comprised of two major parts: a distributed filesystem,
and a parallel execution engine. The filesystem, Hadoop Distributed
Filesystem (HDFS), splits files into blocks which are then spread among
the participating computers. Blocks can be automatically replicated,
such that the loss of any participating computer will not cause data
loss. The execution engine is friendly with HDFS, in that it knows where
file blocks, and there replicas, are located and can place computation on
the same computer that contains the data. The idea here is that data is
larger than the program and that therefore moving the program to the data
will improve efficiency. Hadoop may also start additional computation
tasks that replication the computation being done elsewhere on one of
the replicated blocks.

Basic Hadoop programs require three parts. The first part is some startup
code that tells Hadoop which input files to use, and which map and reduce
functions to execute.  The Hadoop environment will look at the inputs
and decide on the number of mappers to create. Each mapper is responsible
for channeling the data from a file block to the map function, and then
taking that output and passing it onto the reducers. The number of mappers
used is based on two criteria. Each file will get a mapper. If a file
consists of more than one block of data, a mapper will be created for
each additional block. This process is meant to be done automatically,
but has real-world effects on program performance.

\section{Parallel GIS Processing}

Attempts have been made to overcome the limitations of single machine
implementations. Of primary interest are those extending current methods to
use multiple computers.

\subsection{Problems with Desktop Programs on Clusters}

Current desktop approaches to GIS processing such as ArcGIS are unable to make
use of the multi-machine processing environment that compute clusters provide.
While reworking these programs to utilize these extended resources is
possible, it is non-trivial. 

GRASS GIS was reworked\cite{pgrass} to use its collaboration features
to distribute sub-queries among computers. The method described in this
paper utilizes multiple instances of GRASS in a master-slave configuration
where all participants access a shared data repository or filesystem. The
geometries are portioned between the various nodes. Operations are done
on the subsets, and the results are merged to produce the final result.

While the method used to extend GRASS GIS will in fact speed up GIS
processing, it has two flaws. First, GRASS is designed to be used in an
interactive mode rather than a batch or script driven approach.  Second,
the entire set of data used must still fit on a single computer to move
it in or out of the environment.

\subsection{Parallel Databases}

Parallel databases such as TeraData\cite{teradata} and Oracle\cite{oracle}
use data parallel methods. Paradise\cite{paradise, cs-paradise}
spreads data between computers using round-robin, hash, or spatial
partitioning. Because the data is able to be distributed between multiple
computers, the processing is able to scale to larger datasets.

When a query is processed across the database, a task is created
for each fragment of the data. Thus as the data grows larger, the
processing capabilities of the system also increase. If a particular
processing operation requires relatively less computation for each
data record this is fine. However, after the amount of computation per
data record increases beyond a certain point, which is dependent on the
speed of the machine used, the processing operation can be sped up by
utilizing more processors. Major factors in determining how much data
should be processed on each machine, and therefore how the data should
be spread between machines, are memory, computation, and communication
overhead to move the data and computation to another machine. The ratio
of computation to memory and commutation requirements is often referred
to as grain size. Coarser grained processes have more computation per
data record, while finer grained processes have little computation for
each data record.

Databases excel at working with indexed data while allowing multiple
users to interact with the data in a concurrently safe manner through
the use of atomic transactions. The requirements placed upon database
systems to handle these situations slow down computations that don't
utilize indexes or work on an entire dataset at once. The processing
operations this research examines do not require these restrictions,
and as such a more efficient system can be created.

Many universities and research institutions already have significant
investment in compute clusters. These clusters are groups of computer
linked together with high speed network interconnects and high performance
parallel filesystems such as Lustre\cite{lustre}. By separating compute
and storage resources at the cost of a high speed network, compute
clusters are able to separate computation from data storage.

Parallel filesystems allow the data to be separated from the computer
where the processing will be executed by spreading files across
multiple network connected fileservers allowing access that can be
faster than utilizing a computer's local disk for storage while also
enabling processing to spread across the available compute resources
based entirely on the process' grain size.

\subsection{Problems with Current Parallel Approaches}

While desktop GIS programs are relatively easy to use, their current
implementations are fundamentally unable to make use of parallel
computation resources. The attempt to use GRASS was not entirely
successful due to its lack of a non-interactive mode and inability to
work on datasets larger than could be handled by a single computer.

Parallel databases require dedicated resources and expertise to be
useful. Even so, database are limited in their ability to work with a
large range of process granularity as data redistribution is expensive
in terms of rebuilding indexes and configuration changes. Furthermore
optimizing SQL often requires knowledge of the data distribution and
configuration of the database.  Most companies that use databases as a
core technology have dedicated staff to manage these systems.

Chapter 3 details the requirements of a good parallel approach to GIS
processing.
