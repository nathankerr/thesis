\chapter{INTRODUCTION}

% GIS processing is important, but data grows and computation time is too long

Geographic Information Systems (GIS) were designed to model aspects of
the world around us. From roads to temperature, GIS data can be used to
represent a large range of real-world objects, allowing for sophisticated
analysis and processing. Data is usually stored as raster or vector
data. Raster data represents the world as a grid, which each grid cell
containing attributes associated with that area. The grid is georeferenced
and has a specific resolution, which is the amount of space each grid
cell represents. Vector data is comprised of a set of geometries such
as points, lines, or polygons that are georeferenced. Popular forms
of georeferencing include latitude and longitude, zip codes, and street
addresses.

GIS analysis and simulation are used to understand and develop the
environment around us. A common use of GIS data is found in the GPS
based car navigation systems common today. City planners will use GIS
data in applications such as population growth models, land use planning,
and traffic management.

% Data growth (with examples) leads to long runtime or inability to
% process due to memory or processor limitations.

The amount of data to be processed increases as populations grow,
communities become more complex, or the size of the processing area
grows. In the year 2000, Maricopa County, Arizona, mapped 1.2 million
parcels of land in its 9,224 square miles of land. Each parcel of
land is represented as a georeferenced polygon along with various
attribute data such as a unique parcel id, ownership information, and
zoning codes. Parcels of land is just one layer of data that is kept
for Maricopa County. Other layers include roadways, railways, rivers,
schools, voting districts, etc.

When the data required to complete some GIS processing grows beyond the
memory available to a single processor, the processing method must be
adapted to fit within that limitation. A common method is to not load
the entire dataset into memory, but to read it off disk as needed and not
keep what is not. This method works well if the data is only needed once,
as reading from disk is slow.

One application, which is looked at in more detail later, associates
businesses with parcels of land (the data was geocoded differently). The
algorithm is quite simple: for each business, find the nearest parcel of
land with a compatible zoning code. In Maricopa County in the year 2000,
there were 34,302 businesses and 1,218,130 parcels of land. While these
particular datasets fit into memory today, they did not just a few years
ago. Given increased parcel density, or a larger area of land, the memory
capabilities of a standard computer are quickly reached. While there are
specially made computers with extraordinarily large amounts of memory,
they are also extraordinarily expensive.

In addition to memory limitations, the processor also limits the
processing that can be done. In this case 41,784,295,260 comparisons
between businesses and parcels is made. If a computer is able to make one
million comparisons per second, the processing will take approximately 11
hours, 37 minutes. A factor increase in speed to ten million comparisons
per second reduces the time to 1 hour 10 minutes. If only there was a
single computer that would perform 100 billion of these comparisons per
second, then the processing would be done in less than half a second!

As an infinitely fast computer with infinite memory does not exist,
more effort is required to perform this processing in a reasonable time.

\section{Parallel computation methods}

One method of increasing memory capacity and processing capabilities
is to use multiple computers that are inter-connected. By spreading out
the data and processing work, using multiple computers can significantly
reduce the time required to complete the processing as well as increase
the size of data that can be accommodated. There are two main models of
parallel computation, data parallel and task parallel.

% Spread data between computers, increasing memory capabilities

Data parallelism is gained by spreading a dataset across multiple
computers. For example, assuming the 1.2 million parcels of land in
Maricopa County take 1.2GB of RAM, and the available computers have
0.5GB RAM each, three computers can be used to have the entire dataset
in memory. A purely data parallel program would then consist of one of
the computers performing all the processing with the other computers
providing access to the dataset in memory.

% More computers = more processors = doing more at one time

Task parallelism is gained by spreading computation across multiple
computers. For example, if the 34,302 businesses to be processed were
split up between 10 computers, each machine would only need to process
3,430.2 businesses. A purely task parallel program does not need to
interact with any of the other computers, and as such the processing
will be approximately 10 times as fast as running on a single computer,
assuming setup times are negligible.

Many parallel programs are both data parallel and task parallel. By
spreading out both the data and the computation, programs can take full
advantage of the increased memory and processing capabilities of the
computers used.

\section{Challenges with Parallel GIS Processing}

% Overview, full definitions in requirements section

Programming for multiple machines is not the easiest of tasks. A common
paradigm is Single Program, Multiple Data\cite{spmd} (SPMD), uses a
single program that is run on all the computers included in the parallel
computations where each instance of the program operates on different
data, for example a different set of businesses. This paradigm allows
for both task and data parallelism while not being extremely complex,
assuming all program instances follow roughly the same algorithm.

% data distribution - record split, size split, geographically split - quads, by number of records, how to handle overlaps?

Distributing geospatial data between the compute tasks can be accomplished
in several ways. Simply splitting the dataset up by the number of
records it has seems simple, however geometries are not all the same
size, so reading in just the correct records becomes difficult without
preprocessing the data or redistributing records after they are read
in from disk. Splitting the dataset by size is a reasonable approach,
just as long as the records on the boundaries are kept whole. Both these
methods can be used with any record based dataset, geospatial datasets
can also be split up geographically. With a geographic split there are
still many questions to be answered such as how to handle records that
overlap multiple areas, should the dataset be divided into a grid,
or approximately even sized clusters of records?

The purpose of distributing data is to fit in the memory capabilities of
the individual computers while providing the data required for efficient
processing. Of course this becomes more interesting when more than one
dataset is required and they have different requirements.

% Speedup s(n) = t(n)/t(1)
% efficiency e(n) = s(n)/n

The efficiency of a parallel implementation 

% Scalability
% strong scaling = how the solution time varies with the number of processors for a fixed total problem size
% weak scaling = how the solution time varies with the number of processors for a fixed problem size per processor

% Programmability/Flexibility

\section {Alternative Approaches to Parallel GIS Processing}

% Thesis: implement and evaluate.
This thesis implements and evaluates two parallel, dataset centric approaches
to processing large geospatial datasets on clusters. The first approach,
called HadoopGIS, uses the Hadoop\cite{hadoop} map/reduce framework. The
second approach, ClusterGIS, uses the more traditional approach to programming
for clusters, MPI. Both approaches provide the geospatial operations required
by the Open Geospatial Consortium's Simple Features\cite{ogc-sf} standard. By
applying data parallel programming methods and dispensing with record centric
processing methods, both these methods create a fairly easy environment to
program in while providing significant speedup and scaleup.

% Map/Reduce with Hadoop

% Traditional Cluster: MPI

% What has been done/what is lacking. What is needed to constitute a good
% approach. Design of the two approaches. How to evaluate approaches.
% Evaluation. Conclusion.
